{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 10)\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "colours = ['#ff0000','#ff8700','#ffd300','#deff0a','#a1ff0a','#0aff99','#0aefff','#147df5','#580aff','#be0aff','#54478c','#240046']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d617edcb4643e45",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generic_chart(title, x_label, y_label):\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.xticks()\n",
    "    plt.savefig(f'./exact_results/figures/{title}.png')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5409d5107d7986ed",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# read the files\n",
    "dataset_details = pd.read_csv('./exact_results/dataset_details/dataset_details.tsv', sep='\\t', header=0)\n",
    "gold_annotation_types = pd.read_csv('./exact_results/dataset_details/gold_annotation_type_count.tsv', sep='\\t', header=0)\n",
    "evaluation_df = pd.read_csv('./exact_results/eval_log.tsv', sep='\t', header=0).sort_values(by=['prompt_id'])\n",
    "\n",
    "def merge_hallucinations(task):\n",
    "    tsv_files = glob.glob(f'./exact_results/hallucinations/{task}/*.tsv')\n",
    "    \n",
    "    combined_df = pd.DataFrame()\n",
    "    for tsv_file in tsv_files:\n",
    "        df = pd.read_csv(tsv_file, sep='\\t', header=0)\n",
    "        combined_df = pd.concat([combined_df, df])\n",
    "        \n",
    "    return combined_df\n",
    "\n",
    "# hallucinations\n",
    "ner_hallucinations_df = merge_hallucinations('NER')\n",
    "re_hallucinations_df = merge_hallucinations('RE')\n",
    "re_hallucinations_df = re_hallucinations_df.fillna('NA')\n",
    "nerre_hallucinations_df = merge_hallucinations('NERRE')\n",
    "nerre_hallucinations_df = nerre_hallucinations_df.fillna('NA')\n",
    "nerre_hallucinations_df['prompt_id'] = nerre_hallucinations_df['prompt_id'].replace({'p1_one_shot': 'p2_one_shot'})\n",
    "\n",
    "# extracted entities\n",
    "ner_extracted_entities = pd.read_csv('./exact_results/entities/NER/results.tsv', sep='\\t', header=0)\n",
    "\n",
    "re_extracted_entities = pd.read_csv('./exact_results/entities/RE/results.tsv', sep='\\t', header=0)\n",
    "re_extracted_entities = re_extracted_entities.fillna('NA')\n",
    "\n",
    "nerre_extracted_entities = pd.read_csv('./exact_results/entities/NERRE/results.tsv', sep='\\t', header=0)\n",
    "nerre_extracted_entities = nerre_extracted_entities.fillna('NA')\n",
    "nerre_extracted_entities['prompt_id'] = nerre_extracted_entities['prompt_id'].replace({'p1_one_shot': 'p2_one_shot'})\n",
    "\n",
    "# gold\n",
    "ner_gold_entities = pd.read_csv('./exact_results/entities/NER/gold.tsv', sep='\\t', header=0)\n",
    "\n",
    "re_gold_entities = pd.read_csv('./exact_results/entities/RE/gold.tsv', sep='\\t', header=0)\n",
    "re_gold_entities = re_gold_entities.fillna('NA')\n",
    "\n",
    "nerre_gold_entities = pd.read_csv('./exact_results/entities/NERRE/gold.tsv', sep='\\t', header=0)\n",
    "nerre_gold_entities = nerre_gold_entities.fillna('NA')"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Add new column for the language \n",
    "def add_language_col(df):\n",
    "    df['language'] = [\"Spanish prompt\" if '_es'in row.prompt_id  else 'English prompt' for _, row in df.iterrows()]\n",
    "    \n",
    "    return df\n",
    "\n",
    "evaluation_df = add_language_col(evaluation_df)\n",
    "ner_hallucinations_df = add_language_col(ner_hallucinations_df)\n",
    "re_hallucinations_df = add_language_col(re_hallucinations_df)\n",
    "nerre_hallucinations_df = add_language_col(nerre_hallucinations_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8cb2589ed6bd274",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Add formatted prompt id\n",
    "def add_prompt_name(prompt_id):\n",
    "    if \"zero\" in prompt_id:\n",
    "        return \"Zero shot\"\n",
    "    elif \"one\" in prompt_id:\n",
    "        return \"One shot\"\n",
    "    elif \"five\" in prompt_id:\n",
    "        return \"Five shot\"\n",
    "    elif \"ten\" in prompt_id:\n",
    "        return \"Ten shot\"\n",
    "    else: \n",
    "        return prompt_id\n",
    "\n",
    "evaluation_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in evaluation_df.iterrows()]\n",
    "\n",
    "ner_hallucinations_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in ner_hallucinations_df.iterrows()]\n",
    "\n",
    "re_hallucinations_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in re_hallucinations_df.iterrows()]\n",
    "\n",
    "nerre_hallucinations_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in nerre_hallucinations_df.iterrows()]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5349227608c54098",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Performance of the gen LLM based on prompts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f799180b1f9b718"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1.1/2.0 NER performance against different prompts grouped by language \n",
    "temp_df = evaluation_df[evaluation_df['task'] == 'NER']\n",
    "temp_df = temp_df\n",
    "temp_df = temp_df.loc[:, ['formatted_prompt_id', 'precision', 'recall', 'f1', 'language']].sort_values(by=['language']).reset_index(drop=True)\n",
    "\n",
    "eng_temp_df = temp_df[temp_df['language'] == 'English prompt']\n",
    "esp_temp_df = temp_df[temp_df['language'] == 'Spanish prompt']\n",
    "\n",
    "average_eng_val_row = ['Average', eng_temp_df['precision'].mean(), eng_temp_df['recall'].mean(), eng_temp_df['f1'].mean(), 'English prompt']\n",
    "\n",
    "temp_df.loc[len(temp_df)] = average_eng_val_row\n",
    "\n",
    "average_esp_val_row = ['Average', esp_temp_df['precision'].mean(), esp_temp_df['recall'].mean(), esp_temp_df['f1'].mean(), 'Spanish prompt']\n",
    "\n",
    "temp_df.loc[len(temp_df) + 1] = average_esp_val_row\n",
    "\n",
    "temp_df = temp_df.sort_values(by=['language']).reset_index(drop=True)\n",
    "\n",
    "fig = temp_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", color=colours[0:3])\n",
    " \n",
    "sec = fig.secondary_xaxis(location=0)\n",
    "sec.set_xticks([2, 7], labels=['English prompts', 'Spanish prompts'])\n",
    "sec.tick_params('x', length=80, width=0)\n",
    "\n",
    "generic_chart(f'Evaluation metrics for NER grouped by prompt language', 'Prompts', 'Score') "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c677fbc8ce2a80f4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1.2, 1.3 F1, precision, recall for NER, RE, NERRE seperated by prompts, add overall average of metrics \n",
    "\n",
    "def generate_evalutation_metrics(task_type):\n",
    "    temp_df = evaluation_df[evaluation_df['task'] == task_type]\n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'precision', 'recall', 'f1']].reset_index(drop=True)\n",
    "    \n",
    "    average_val_row = ['Average', temp_df['precision'].mean(), temp_df['recall'].mean(), temp_df['f1'].mean()]\n",
    "\n",
    "    temp_df.loc[len(temp_df)] = average_val_row\n",
    "    \n",
    "    temp_df.plot(x=\"formatted_prompt_id\", y=[\"precision\", \"recall\", \"f1\"], kind=\"bar\", color=colours[0:3]) \n",
    "    generic_chart(f'Evaluation metrics for {task_type}', 'Prompts', 'Score') \n",
    "\n",
    "generate_evalutation_metrics('RE')\n",
    "generate_evalutation_metrics('NERRE')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a99efa9e9aa7232d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exploration of the hallucinations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23a0e5fab434ae52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 3. Per task --> Hallucinations per prompt stacked with entities extracted\n",
    "\n",
    "# NER based on prompt language\n",
    "temp_ner_hall_df = evaluation_df[evaluation_df['task'] == 'NER']\n",
    "\n",
    "temp_ner_hall_df = temp_ner_hall_df.loc[:, ['formatted_prompt_id', 'extracted_tuples_or_triplets_per_prompt', 'tuple_or_triplet_hallucinations_per_prompt', 'language']].sort_values(by=['language']).reset_index(drop=True)\n",
    "\n",
    "eng_ner_hall_temp_df = temp_ner_hall_df[temp_ner_hall_df['language'] == 'English prompt']\n",
    "esp_ner_hall_temp_df = temp_ner_hall_df[temp_ner_hall_df['language'] == 'Spanish prompt']\n",
    "\n",
    "ner_en_hall_average_val_row = ['Average', math.ceil(eng_ner_hall_temp_df['extracted_tuples_or_triplets_per_prompt'].mean()), math.ceil(eng_ner_hall_temp_df ['tuple_or_triplet_hallucinations_per_prompt'].mean()), 'English prompt']\n",
    "\n",
    "temp_ner_hall_df.loc[len(temp_ner_hall_df)] = ner_en_hall_average_val_row\n",
    "\n",
    "ner_es_hall_average_val_row = ['Average', math.ceil(esp_ner_hall_temp_df['extracted_tuples_or_triplets_per_prompt'].mean()), math.ceil(esp_ner_hall_temp_df ['tuple_or_triplet_hallucinations_per_prompt'].mean()), 'Spanish prompt']\n",
    "\n",
    "temp_ner_hall_df.loc[len(temp_ner_hall_df) + 1] = ner_es_hall_average_val_row\n",
    "\n",
    "temp_ner_hall_df = temp_ner_hall_df.sort_values(by=['language']).reset_index(drop=True)\n",
    "\n",
    "fig = temp_ner_hall_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", stacked=True, color=colours[4:6])\n",
    " \n",
    "sec = fig.secondary_xaxis(location=0)\n",
    "sec.set_xticks([2, 7], labels=['English prompts', 'Spanish prompts'])\n",
    "sec.tick_params('x', length=80, width=0)\n",
    "\n",
    "plt.legend([\"Extracted instances\", \"Hallucinated instances\"])\n",
    "generic_chart(f'Instances for NER', 'Prompts', 'Instances')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae154835a23641de",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_stacked_entity_graph(task_type):\n",
    "    temp_df = evaluation_df[evaluation_df['task'] == task_type]\n",
    "    \n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'extracted_tuples_or_triplets_per_prompt', 'tuple_or_triplet_hallucinations_per_prompt']].reset_index(drop=True)\n",
    "    \n",
    "    average_val_row = ['Average', temp_df['extracted_tuples_or_triplets_per_prompt'].mean(), temp_df['tuple_or_triplet_hallucinations_per_prompt'].mean()]\n",
    "\n",
    "    temp_df.loc[len(temp_df)] = average_val_row\n",
    "    \n",
    "    x1 = temp_df[\"formatted_prompt_id\"].tolist()\n",
    "    y1 = temp_df[\"extracted_tuples_or_triplets_per_prompt\"].tolist()\n",
    "    y2 = temp_df[\"tuple_or_triplet_hallucinations_per_prompt\"].tolist()\n",
    "    \n",
    "    plt.bar(x1, y1, color=colours[4])\n",
    "    plt.bar(x1, y2, bottom=y1, color=colours[5])\n",
    "    \n",
    "    plt.legend([\"Extracted instances\", \"Hallucinated instances\"])\n",
    "    generic_chart(f'Instances for {task_type}', 'Prompts', 'Instances')\n",
    "\n",
    "# TODO: Add a note specifying that NER shows the span-label tuple instances while RE and NERRE shows spans-labels-relation triplet instances\n",
    "generate_stacked_entity_graph('RE')\n",
    "generate_stacked_entity_graph('NERRE')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c82e46f39e6a9e02",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 4. Hallucinations broken down by type of the hallucination (basically looking at the over generation of the found instances and fabrication)\n",
    "\n",
    "# stacked -1 and -2\n",
    "def order_hallucinations_by_type(task_type, df):\n",
    "    offset_col_name = 'offset1'\n",
    "    if task_type == 'RE' or task_type == 'NERRE':\n",
    "        offset_col_name = 'offset1_start'\n",
    "    hallucinations_by_type_df = pd.DataFrame(df.groupby('prompt_id')[offset_col_name].value_counts()).reset_index()\n",
    "    \n",
    "    hallucinations_by_type_df_fabrications = hallucinations_by_type_df[hallucinations_by_type_df[offset_col_name] == -1].reset_index(drop=True).sort_values(by='prompt_id')\n",
    "    hallucinations_by_type_df_fabrications = hallucinations_by_type_df_fabrications.rename(columns={'count': 'Fabrications'})\n",
    "    \n",
    "    hallucinations_by_type_df_over_generated = hallucinations_by_type_df[hallucinations_by_type_df[offset_col_name] == -2].reset_index(drop=True).sort_values(by='prompt_id')\n",
    "    hallucinations_by_type_df_over_generated = hallucinations_by_type_df_over_generated.rename(columns={'count': 'Over generated'})\n",
    "    \n",
    "    cols = hallucinations_by_type_df_over_generated.columns.difference(hallucinations_by_type_df_fabrications.columns)\n",
    "    \n",
    "    refactored_hallucinations_by_type_df = pd.merge(hallucinations_by_type_df_fabrications, hallucinations_by_type_df_over_generated[cols], left_index=True, right_index=True, how='outer')\n",
    "    \n",
    "    if task_type == 'NER':\n",
    "        refactored_hallucinations_by_type_df = add_language_col(refactored_hallucinations_by_type_df)\n",
    "    \n",
    "    refactored_hallucinations_by_type_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in refactored_hallucinations_by_type_df.iterrows()]\n",
    "    \n",
    "    if task_type == 'NER':\n",
    "        refactored_hallucinations_by_type_df = refactored_hallucinations_by_type_df.sort_values(by='language')\n",
    "    \n",
    "    refactored_hallucinations_by_type_df = refactored_hallucinations_by_type_df.reset_index(drop=True)\n",
    "    \n",
    "    refactored_hallucinations_by_type_df = refactored_hallucinations_by_type_df.loc[:, ['prompt_id', 'formatted_prompt_id', 'Over generated', 'Fabrications']]\n",
    "    fig = refactored_hallucinations_by_type_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", stacked=True, color=colours[6:8])\n",
    "    \n",
    "    if task_type == 'NER': \n",
    "        sec = fig.secondary_xaxis(location=0)\n",
    "        sec.set_xticks([2, 6], labels=['English prompts', 'Spanish prompts'])\n",
    "        sec.tick_params('x', length=80, width=0)\n",
    "        \n",
    "    plt.legend([\"Over generated instances\", \"Fabricated instances\"])\n",
    "    generic_chart(f'Hallucinations by type for {task_type}', 'Prompts', 'Hallucinated instances')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "325e6015dcfd7f4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "order_hallucinations_by_type('NER', ner_hallucinations_df)\n",
    "order_hallucinations_by_type('RE', re_hallucinations_df)\n",
    "order_hallucinations_by_type('NERRE', nerre_hallucinations_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac42bc5f136a34c6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 5. Hallucinations broken down by the type of the entities and relations\n",
    "def broken_by_entity_type_ner(task_type, df, df_type):\n",
    "    breakdown_by_entity_type_df = pd.DataFrame(df.groupby('prompt_id')['label'].value_counts()).reset_index()\n",
    "    \n",
    "    unique_labels = breakdown_by_entity_type_df['label'].unique().tolist()\n",
    "    unique_prompts = breakdown_by_entity_type_df['prompt_id'].unique().tolist()\n",
    "    \n",
    "    new_cols = ['prompt_id'] + unique_labels\n",
    "    \n",
    "    new_breakdown_by_entity_type_df = pd.DataFrame(columns=new_cols)\n",
    "    for prompt in unique_prompts:\n",
    "        df_row = [prompt]\n",
    "        for label in unique_labels:\n",
    "            # find the count from the df \n",
    "            value = 0\n",
    "            value_tuple =  breakdown_by_entity_type_df[(breakdown_by_entity_type_df['prompt_id'] == prompt) & (breakdown_by_entity_type_df['label'] == label)]['count']\n",
    "            \n",
    "            if not value_tuple.empty:\n",
    "                value = value_tuple.item()\n",
    "                \n",
    "            df_row = df_row + [value]\n",
    "            \n",
    "        new_breakdown_by_entity_type_df.loc[len(new_breakdown_by_entity_type_df)] = df_row\n",
    "    \n",
    "    if task_type == 'NER':\n",
    "        new_breakdown_by_entity_type_df = add_language_col(new_breakdown_by_entity_type_df)\n",
    "\n",
    "    new_breakdown_by_entity_type_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in new_breakdown_by_entity_type_df.iterrows()]\n",
    "\n",
    "    if task_type == 'NER':\n",
    "        new_breakdown_by_entity_type_df = new_breakdown_by_entity_type_df.sort_values(by='language')\n",
    "\n",
    "    new_breakdown_by_entity_type_df = new_breakdown_by_entity_type_df.reset_index(drop=True)\n",
    "    \n",
    "    new_cols = ['formatted_prompt_id'] + unique_labels\n",
    "    new_breakdown_by_entity_type_df = new_breakdown_by_entity_type_df.loc[:, new_cols]\n",
    "    fig = new_breakdown_by_entity_type_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", stacked=True, color=colours)\n",
    "    \n",
    "    if task_type == 'NER': \n",
    "        sec = fig.secondary_xaxis(location=0)\n",
    "        sec.set_xticks([2, 6], labels=['English prompts', 'Spanish prompts'])\n",
    "        sec.tick_params('x', length=80, width=0)\n",
    "        \n",
    "    plt.legend(unique_labels)\n",
    "    generic_chart(f'{df_type} by entity type for {task_type}', 'Prompts', 'Entity count')\n",
    "    \n",
    "broken_by_entity_type_ner('NER', ner_hallucinations_df, 'Hallucinations')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a588fdd8cf856fc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def broken_by_entity_type_re(task_type, df, df_type):\n",
    "    \n",
    "    unique_labels1 = df['label1'].unique().tolist()\n",
    "    unique_labels2 = df['label2'].unique().tolist()\n",
    "    unique_labels = unique_labels1 + unique_labels2\n",
    "    unique_labels = list(set(unique_labels))\n",
    "    unique_prompts = df['prompt_id'].unique().tolist()\n",
    "    \n",
    "    new_cols = ['prompt_id'] + unique_labels\n",
    "    \n",
    "    label1 = pd.DataFrame(df.groupby('prompt_id')['label1'].value_counts()).reset_index()\n",
    "    label2 = pd.DataFrame(df.groupby('prompt_id')['label2'].value_counts()).reset_index()\n",
    "        \n",
    "    new_hallucinations_by_entity_type_df = pd.DataFrame(columns=new_cols)\n",
    "    for prompt in unique_prompts:\n",
    "        df_row = [prompt]\n",
    "        for label in unique_labels:\n",
    "            # find the count from the df \n",
    "            value = 0\n",
    "            value_tuple_label1 = label1[(label1['prompt_id'] == prompt) & (label1['label1'] == label)]['count']\n",
    "            \n",
    "            value_tuple_label2 = label2[(label2['prompt_id'] == prompt) & (label2['label2'] == label)]['count']\n",
    "                        \n",
    "            if not value_tuple_label1.empty:\n",
    "                value = value_tuple_label1.item()\n",
    "            \n",
    "            if not value_tuple_label2.empty:\n",
    "                value = value + value_tuple_label2.item()\n",
    "                \n",
    "            df_row = df_row + [value]\n",
    "            \n",
    "        new_hallucinations_by_entity_type_df.loc[len(new_hallucinations_by_entity_type_df)] = df_row\n",
    "\n",
    "    new_hallucinations_by_entity_type_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in new_hallucinations_by_entity_type_df.iterrows()]\n",
    "\n",
    "\n",
    "    new_hallucinations_by_entity_type_df = new_hallucinations_by_entity_type_df.sort_values(by='prompt_id').reset_index(drop=True)\n",
    "    \n",
    "    new_cols = ['formatted_prompt_id'] + unique_labels\n",
    "    new_hallucinations_by_entity_type_df = new_hallucinations_by_entity_type_df.loc[:, new_cols]\n",
    "    new_hallucinations_by_entity_type_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", stacked=True, color=colours)\n",
    "        \n",
    "    plt.legend(unique_labels)\n",
    "    generic_chart(f'{df_type} by entity type for {task_type}', 'Prompts', 'Entity count')\n",
    "    \n",
    "broken_by_entity_type_re('RE', re_hallucinations_df, 'Hallucinations')\n",
    "broken_by_entity_type_re('NERRE', nerre_hallucinations_df, 'Hallucinations')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e24aba6aa1345d4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def broken_by_relation_type(task_type, df, df_type):\n",
    "    unique_relations = df['relation_type'].unique().tolist()\n",
    "    unique_prompts = df['prompt_id'].unique().tolist()\n",
    "    \n",
    "    new_cols = ['prompt_id'] + unique_relations\n",
    "    \n",
    "    relations_df = pd.DataFrame(df.groupby('prompt_id')['relation_type'].value_counts(dropna=False)).reset_index()\n",
    "        \n",
    "    broken_by_relation_type_df = pd.DataFrame(columns=new_cols)\n",
    "    for prompt in unique_prompts:\n",
    "        df_row = [prompt]\n",
    "        for relation in unique_relations:\n",
    "            # find the count from the df\n",
    "            value = 0\n",
    "            relation_value = relations_df[(relations_df['prompt_id'] == prompt) & (relations_df['relation_type'] == relation)]['count']\n",
    "                        \n",
    "            if not relation_value.empty:\n",
    "                value = relation_value.item()\n",
    "                \n",
    "            df_row = df_row + [value]\n",
    "            \n",
    "        broken_by_relation_type_df.loc[len(broken_by_relation_type_df)] = df_row\n",
    "    broken_by_relation_type_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in broken_by_relation_type_df.iterrows()]\n",
    "\n",
    "    broken_by_relation_type_df = broken_by_relation_type_df.sort_values(by='prompt_id').reset_index(drop=True)\n",
    "        \n",
    "    new_cols = ['formatted_prompt_id'] + unique_relations\n",
    "    \n",
    "    broken_by_relation_type_df = broken_by_relation_type_df.loc[:, new_cols]\n",
    "    broken_by_relation_type_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", stacked=True, color=colours)\n",
    "        \n",
    "    plt.legend(unique_relations)\n",
    "    generic_chart(f'{df_type} by relation type for {task_type}', 'Prompts', 'Relation count')\n",
    "    \n",
    "broken_by_relation_type('RE', re_hallucinations_df, 'Hallucinations')\n",
    "broken_by_relation_type('NERRE', nerre_hallucinations_df, 'Hallucinations')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "306102e9e7c22dff",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Classification of the extracted entities and identified relations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11016f6ebbbbb1b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 6. true positives, false negatives, false positives, false positive relations, false negative relations (stacked) vs total extracted entities for each task for exact match vs relaxed match\n",
    "\n",
    "# for ner\n",
    "entity_division_ner_df = evaluation_df[evaluation_df['task'] == 'NER']\n",
    "entity_division_ner_df = entity_division_ner_df.loc[:, ['formatted_prompt_id', 'true_positive', 'false_positive', 'false_negative', 'language']].sort_values(by=['language']).reset_index(drop=True)\n",
    "\n",
    "eng_entity_division_ner_df = entity_division_ner_df[entity_division_ner_df['language'] == 'English prompt']\n",
    "esp_entity_division_ner_df = entity_division_ner_df[entity_division_ner_df['language'] == 'Spanish prompt']\n",
    "\n",
    "average_eng_val_row = ['Average', eng_entity_division_ner_df['true_positive'].mean(), eng_entity_division_ner_df['false_positive'].mean(), eng_entity_division_ner_df['false_negative'].mean(),'English prompt']\n",
    "\n",
    "entity_division_ner_df.loc[len(entity_division_ner_df)] = average_eng_val_row\n",
    "\n",
    "average_esp_val_row = ['Average', esp_entity_division_ner_df['true_positive'].mean(), esp_entity_division_ner_df['false_positive'].mean(), esp_entity_division_ner_df['false_negative'].mean(),'Spanish prompt']\n",
    "\n",
    "entity_division_ner_df.loc[len(entity_division_ner_df) + 1] = average_esp_val_row\n",
    "\n",
    "entity_division_ner_df = entity_division_ner_df.sort_values(by=['language']).reset_index(drop=True)\n",
    "\n",
    "fig = entity_division_ner_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", color=colours[5:8])\n",
    " \n",
    "sec = fig.secondary_xaxis(location=0)\n",
    "sec.set_xticks([2, 6], labels=['English prompts', 'Spanish prompts'])\n",
    "sec.tick_params('x', length=80, width=0)\n",
    "\n",
    "plt.legend(['true_positive', 'false_positive', 'false_negative'])\n",
    "\n",
    "generic_chart(f'Entity division for NER grouped by prompt language', 'Prompts', 'Count') "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "863341c889aa9721",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# for relation triplets \n",
    "def entity_division_with_relations(task):\n",
    "    entity_division_df = evaluation_df[evaluation_df['task'] == task]\n",
    "    \n",
    "    entity_division_df = entity_division_df.loc[:, ['formatted_prompt_id', 'true_positive', 'false_positive', 'false_negative', 'false_positive_relations','false_negative_relations']].reset_index(drop=True)\n",
    "    \n",
    "    average_row = ['Average', entity_division_df['true_positive'].mean(), entity_division_df['false_positive'].mean(), entity_division_df['false_negative'].mean(), \n",
    "     entity_division_df['false_positive_relations'].mean(), entity_division_df['false_negative_relations'].mean()]\n",
    "    \n",
    "    entity_division_df.loc[len(entity_division_df)] = average_row\n",
    "    \n",
    "    entity_division_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", color=colours[5:10])\n",
    "    \n",
    "    plt.legend(['true_positive', 'false_positive', 'false_negative', 'false_positive_relations','false_negative_relations'])\n",
    "    \n",
    "    generic_chart(f'Entity and relation division for {task} grouped by prompt language', 'Prompts', 'Count') \n",
    "    \n",
    "entity_division_with_relations('RE')\n",
    "entity_division_with_relations('NERRE')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "868fa79d4f628083",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 7. Breakdown of the gold entities and relations vs extracted entities and relations for each task\n",
    "\n",
    "broken_by_entity_type_ner('NER', ner_extracted_entities, 'Extracted entities')\n",
    "broken_by_entity_type_re('RE', re_extracted_entities, 'Extracted entities')\n",
    "broken_by_entity_type_re('NERRE', nerre_extracted_entities, 'Extracted entities')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18df85ba4b178545",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "broken_by_relation_type('RE', re_extracted_entities, 'Extracted entities')\n",
    "broken_by_relation_type('NERRE', nerre_extracted_entities, 'Extracted entities')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e7d58cb61375432",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def gold_broken_by_entity_type_ner(task_type, df, df_type):\n",
    "    breakdown_by_entity_type = df['label'].value_counts()    \n",
    "    breakdown_by_entity_type.plot(x=\"label\", kind=\"bar\", color=colours)\n",
    "        \n",
    "    generic_chart(f'{df_type} types for {task_type}', 'Type', 'Entity count')\n",
    "    \n",
    "gold_broken_by_entity_type_ner('NER', ner_gold_entities, 'Gold entity')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96ac2044d15c4b3f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def gold_broken_by_entity_type_re(task_type, df, df_type):\n",
    "    unique_labels1 = df['label1'].unique().tolist()\n",
    "    unique_labels2 = df['label2'].unique().tolist()\n",
    "    unique_labels = unique_labels1 + unique_labels2\n",
    "    unique_labels = list(set(unique_labels))\n",
    "    \n",
    "    new_cols = unique_labels\n",
    "    \n",
    "    label1 = pd.DataFrame(df['label1'].value_counts()).reset_index()\n",
    "    label1.columns = ['label', 'count']\n",
    "    label2 = pd.DataFrame(df['label2'].value_counts()).reset_index()\n",
    "    label2.columns = ['label', 'count']\n",
    "    breakdown_by_entity_type = pd.DataFrame(columns=new_cols)\n",
    "    \n",
    "    df_row = []\n",
    "    for label in unique_labels:\n",
    "        value = 0\n",
    "        value_tuple_label1 = label1[(label1['label'] == label)]['count']\n",
    "        \n",
    "        value_tuple_label2 = label2[(label2['label'] == label)]['count']\n",
    "                    \n",
    "        if not value_tuple_label1.empty:\n",
    "            value = value_tuple_label1.item()\n",
    "        \n",
    "        if not value_tuple_label2.empty:\n",
    "            value = value + value_tuple_label2.item()\n",
    "        df_row = df_row + [value] \n",
    "    breakdown_by_entity_type.loc[len(breakdown_by_entity_type)] = df_row\n",
    "    \n",
    "    breakdown_by_entity_type.plot(kind=\"bar\", color=colours)\n",
    "        \n",
    "    generic_chart(f'{df_type} types for {task_type}', 'Entity type', 'Entity count')\n",
    "    \n",
    "gold_broken_by_entity_type_re('RE', re_gold_entities, 'Gold entity')\n",
    "gold_broken_by_entity_type_re('NERRE', nerre_gold_entities, 'Gold entity')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a28e9c9d50794b6a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def gold_broken_by_relation_type(task_type, df, df_type):\n",
    "    breakdown_by_entity_type = df['relation_type'].value_counts(dropna=False) \n",
    "    \n",
    "    breakdown_by_entity_type.plot(x=\"relation_type\", kind=\"bar\", color=colours)\n",
    "        \n",
    "    generic_chart(f'{df_type} types for {task_type}', 'Type', 'Relation count')\n",
    "    \n",
    "gold_broken_by_relation_type('RE', re_gold_entities, 'Gold relation')\n",
    "gold_broken_by_relation_type('NERRE', nerre_gold_entities, 'Gold relation')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b198f8244af5baa",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
