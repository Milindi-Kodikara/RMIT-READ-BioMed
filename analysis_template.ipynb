{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63618d0b74432daa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79ae8b6789d05d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617edcb4643e45",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.329126Z",
     "iopub.status.busy": "2024-12-13T11:02:08.328828Z",
     "iopub.status.idle": "2024-12-13T11:02:08.653390Z",
     "shell.execute_reply": "2024-12-13T11:02:08.653122Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 10)\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "colours = ['#ff0000', '#ff8700', '#ffd300', '#deff0a', '#a1ff0a', '#0aff99', '#0aefff', '#147df5', '#580aff', '#be0aff',\n",
    "           '#54478c', '#240046']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b72d912b00c0e",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.655022Z",
     "iopub.status.busy": "2024-12-13T11:02:08.654843Z",
     "iopub.status.idle": "2024-12-13T11:02:08.656631Z",
     "shell.execute_reply": "2024-12-13T11:02:08.656443Z"
    }
   },
   "outputs": [],
   "source": [
    "result_folder_path = os.environ[\"RESULT-FOLDER-PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409d5107d7986ed",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.657660Z",
     "iopub.status.busy": "2024-12-13T11:02:08.657583Z",
     "iopub.status.idle": "2024-12-13T11:02:08.659199Z",
     "shell.execute_reply": "2024-12-13T11:02:08.658995Z"
    }
   },
   "outputs": [],
   "source": [
    "def generic_chart(title, x_label, y_label):\n",
    "    # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    # plt.subplots_adjust(right=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label, labelpad=55)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(f'{result_folder_path}/results/figures/{title}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb143f9654d3c1ff",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.660257Z",
     "iopub.status.busy": "2024-12-13T11:02:08.660181Z",
     "iopub.status.idle": "2024-12-13T11:02:08.661932Z",
     "shell.execute_reply": "2024-12-13T11:02:08.661741Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_hallucinations(task):\n",
    "    tsv_files = glob.glob(f'{result_folder_path}/results/hallucinations/{task}/*.tsv')\n",
    "    \n",
    "    combined_df = pd.DataFrame()\n",
    "    for tsv_file in tsv_files:\n",
    "        df = pd.read_csv(tsv_file, sep='\\t', header=0)\n",
    "        combined_df = pd.concat([combined_df, df])\n",
    "        \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d217f6e143cbf",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.662948Z",
     "iopub.status.busy": "2024-12-13T11:02:08.662872Z",
     "iopub.status.idle": "2024-12-13T11:02:08.664481Z",
     "shell.execute_reply": "2024-12-13T11:02:08.664300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add new column for the language \n",
    "def add_language_col(df):\n",
    "    df['language'] = [\"Spanish prompt\" if '_es'in row.prompt_id  else 'English prompt' for _, row in df.iterrows()]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e75fdb5caacff",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.665490Z",
     "iopub.status.busy": "2024-12-13T11:02:08.665420Z",
     "iopub.status.idle": "2024-12-13T11:02:08.667321Z",
     "shell.execute_reply": "2024-12-13T11:02:08.667105Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add formatted prompt id\n",
    "def add_prompt_name(prompt_id):\n",
    "    prompt_label = prompt_id\n",
    "    if \"zero\" in prompt_id:\n",
    "        prompt_label = \"0\"\n",
    "    elif \"one\" in prompt_id:\n",
    "        prompt_label = \"1\"\n",
    "    elif \"five\" in prompt_id:\n",
    "        prompt_label = \"5\"\n",
    "    elif \"ten\" in prompt_id:\n",
    "        prompt_label = \"10\"\n",
    "        \n",
    "    if \"output\" in prompt_id:\n",
    "        prompt_label = prompt_label + \"_output\"\n",
    "        \n",
    "    if \"guideline\" in prompt_id:\n",
    "        prompt_label = prompt_label + \"_guideline\"\n",
    "        \n",
    "    if \"all\" in prompt_id:\n",
    "        prompt_label = prompt_label + \"_output_guideline\"\n",
    "        \n",
    "    if '_es'in prompt_id:\n",
    "        prompt_label = prompt_label + \"_es\"\n",
    "    else:\n",
    "        prompt_label = prompt_label + \"_en\"\n",
    "    \n",
    "    return prompt_label"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def add_new_cols(df):\n",
    "    df['shots'] = [int(row.formatted_prompt_id.split(\"_\")[0]) for _, row in df.iterrows()]\n",
    "    df['guideline'] = [\"Guideline defined\" if \"guideline\" in row.formatted_prompt_id else \"Guideline undefined\" for _, row in df.iterrows()]\n",
    "    df['output'] = [\"Output defined\" if \"output\" in row.formatted_prompt_id else \"Output undefined\" for _, row in df.iterrows()]\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "614fb032820c6c09",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c677fbc8ce2a80f4",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.668430Z",
     "iopub.status.busy": "2024-12-13T11:02:08.668359Z",
     "iopub.status.idle": "2024-12-13T11:02:08.671251Z",
     "shell.execute_reply": "2024-12-13T11:02:08.671037Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1.1/2.0 NER performance against different prompts grouped by language \n",
    "def generate_cross_linguistic_evaluation_metrics(task_type):\n",
    "    temp_df = evaluation_df[evaluation_df['task'] == task_type]\n",
    "    temp_df = temp_df\n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'precision', 'recall', 'f1', 'language']].sort_values(\n",
    "        by=['language']).reset_index(drop=True)\n",
    "\n",
    "    eng_temp_df = temp_df[temp_df['language'] == 'English prompt']\n",
    "    esp_temp_df = temp_df[temp_df['language'] == 'Spanish prompt']\n",
    "\n",
    "    average_eng_val_row = ['Avg', eng_temp_df['precision'].mean(), eng_temp_df['recall'].mean(), eng_temp_df['f1'].mean(), 'English prompt']\n",
    "\n",
    "    temp_df.loc[len(temp_df)] = average_eng_val_row\n",
    "\n",
    "    average_esp_val_row = ['Avg', esp_temp_df['precision'].mean(), esp_temp_df['recall'].mean(), esp_temp_df['f1'].mean(), 'Spanish prompt']\n",
    "\n",
    "    temp_df.loc[len(temp_df) + 1] = average_esp_val_row\n",
    "    \n",
    "    temp_df['shots'] = [20 if row.formatted_prompt_id.split(\"_\")[0] == \"Avg\" else int(row.formatted_prompt_id.split(\"_\")[0]) for _, row in temp_df.iterrows()]\n",
    "    \n",
    "    temp_df['formatted_prompt_id'] = [row.formatted_prompt_id.replace(\"_es\", \"\")  for _, row in temp_df.iterrows()]\n",
    "    \n",
    "    temp_df['formatted_prompt_id'] = [row.formatted_prompt_id.replace(\"_en\", \"\")  for _, row in temp_df.iterrows()]\n",
    "\n",
    "    temp_df = temp_df.sort_values(by=['language', 'shots', 'formatted_prompt_id'], ascending=[True, True, True]).reset_index(drop=True)\n",
    "    \n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'precision', 'recall', 'f1', 'language']]\n",
    "\n",
    "    fig = temp_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", color=colours[0:3])\n",
    "\n",
    "    sec = fig.secondary_xaxis(location=0)\n",
    "    sec.set_xticks([8.5, 25.5], labels=['English prompt (shots)', 'Spanish prompt (shots)'])\n",
    "    sec.tick_params('x', length=275, width=0)\n",
    "\n",
    "    generic_chart(f'Results for varying number of shots for NER, grouped by prompt language', 'Prompts', 'Score') "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_cross_linguistic_f1_shots(task_type):\n",
    "    temp_df = evaluation_df[evaluation_df['task'] == task_type]\n",
    "    temp_df = temp_df\n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'f1', 'language']].sort_values(\n",
    "        by=['language']).reset_index(drop=True)\n",
    "\n",
    "    eng_temp_df = temp_df[temp_df['language'] == 'English prompt']\n",
    "    esp_temp_df = temp_df[temp_df['language'] == 'Spanish prompt']\n",
    "\n",
    "    average_eng_val_row = ['Avg', eng_temp_df['f1'].mean(), 'English prompt']\n",
    "\n",
    "    temp_df.loc[len(temp_df)] = average_eng_val_row\n",
    "\n",
    "    average_esp_val_row = ['Avg', esp_temp_df['f1'].mean(), 'Spanish prompt']\n",
    "\n",
    "    temp_df.loc[len(temp_df) + 1] = average_esp_val_row\n",
    "\n",
    "    temp_df['shots'] = [\n",
    "        20 if row.formatted_prompt_id.split(\"_\")[0] == \"Avg\" else int(row.formatted_prompt_id.split(\"_\")[0]) for _, row\n",
    "        in temp_df.iterrows()]\n",
    "\n",
    "    temp_df = temp_df.sort_values(by=['language', 'shots', 'formatted_prompt_id'],\n",
    "                                  ascending=[True, True, True]).reset_index(drop=True)\n",
    "\n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'f1', 'language']]\n",
    "\n",
    "    column_color = [colours[0] if row.formatted_prompt_id == 'Avg' else (\n",
    "        colours[10] if '_es' in row.formatted_prompt_id else colours[11]) for _, row in temp_df.iterrows()]\n",
    "    \n",
    "    temp_df['formatted_prompt_id'] = [row.formatted_prompt_id.replace(\"_es\", \"\") for _, row in temp_df.iterrows()]\n",
    "\n",
    "    temp_df['formatted_prompt_id'] = [row.formatted_prompt_id.replace(\"_en\", \"\") for _, row in temp_df.iterrows()]\n",
    "\n",
    "    fig = temp_df.plot(x=\"formatted_prompt_id\", y=\"f1\", kind=\"bar\", color=column_color, legend=False)\n",
    "\n",
    "    sec = fig.secondary_xaxis(location=0)\n",
    "    sec.set_xticks([8.5, 25.5], labels=['English prompt (shots)', 'Spanish prompt (shots)'])\n",
    "    sec.tick_params('x', length=275, width=0)\n",
    "\n",
    "    generic_chart(f'F1 for varying number of shots for NER, grouped by prompt language', 'Prompts', 'F1 Score')\n",
    "\n",
    "    print(temp_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "364c73824cab00a9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_cross_linguistic_f1_guideline(task_type):\n",
    "    temp_df = evaluation_df[evaluation_df['task'] == task_type]\n",
    "    temp_df = temp_df\n",
    "    temp_df = add_new_cols(temp_df)\n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'f1', 'guideline']].sort_values(\n",
    "        by=['guideline']).reset_index(drop=True)\n",
    "\n",
    "    eng_temp_df = temp_df[temp_df['guideline'] == 'Guideline defined']\n",
    "    esp_temp_df = temp_df[temp_df['guideline'] == 'Guideline undefined']\n",
    "\n",
    "    average_eng_val_row = ['Avg', eng_temp_df['f1'].mean(), 'Guideline defined']\n",
    "\n",
    "    temp_df.loc[len(temp_df)] = average_eng_val_row\n",
    "\n",
    "    average_esp_val_row = ['Avg', esp_temp_df['f1'].mean(), 'Guideline undefined']\n",
    "\n",
    "    temp_df.loc[len(temp_df) + 1] = average_esp_val_row\n",
    "    \n",
    "    temp_df['shots'] = [20 if row.formatted_prompt_id.split(\"_\")[0] == \"Avg\" else int(row.formatted_prompt_id.split(\"_\")[0]) for _, row in temp_df.iterrows()]\n",
    "\n",
    "    temp_df = temp_df.sort_values(by=['guideline', 'shots', 'formatted_prompt_id'], ascending=[True, True, True]).reset_index(drop=True)\n",
    "    \n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'f1', 'guideline']]\n",
    "    \n",
    "    temp_df['formatted_prompt_id'] = [row.formatted_prompt_id.replace(\"_guideline\", \"\")  for _, row in temp_df.iterrows()]\n",
    "    \n",
    "    column_color = [colours[0] if row.formatted_prompt_id == 'Avg' else (\n",
    "        colours[8] if '_es' in row.formatted_prompt_id else colours[9]) for _, row in temp_df.iterrows()]\n",
    "    \n",
    "    fig = temp_df.plot(x=\"formatted_prompt_id\", y=\"f1\", kind=\"bar\", color=column_color, legend=False)\n",
    "\n",
    "    sec = fig.secondary_xaxis(location=0)\n",
    "    sec.set_xticks([8.5, 25.5], labels=['Guideline defined', 'Guideline undefined'])\n",
    "    sec.tick_params('x', length=275, width=0)\n",
    "\n",
    "    generic_chart(f'F1 for NER, grouped by the conditional provision of the annotation guideline', 'Prompts', 'F1 Score') \n",
    "    print(temp_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67defe592fa27b54",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_cross_linguistic_f1_output(task_type):\n",
    "    temp_df = evaluation_df[evaluation_df['task'] == task_type]\n",
    "    temp_df = temp_df\n",
    "    temp_df = add_new_cols(temp_df)\n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'f1', 'output']].sort_values(\n",
    "        by=['output']).reset_index(drop=True)\n",
    "\n",
    "    eng_temp_df = temp_df[temp_df['output'] == 'Output defined']\n",
    "    esp_temp_df = temp_df[temp_df['output'] == 'Output undefined']\n",
    "\n",
    "    average_eng_val_row = ['Avg', eng_temp_df['f1'].mean(), 'Output defined']\n",
    "\n",
    "    temp_df.loc[len(temp_df)] = average_eng_val_row\n",
    "\n",
    "    average_esp_val_row = ['Avg', esp_temp_df['f1'].mean(), 'Output undefined']\n",
    "\n",
    "    temp_df.loc[len(temp_df) + 1] = average_esp_val_row\n",
    "    \n",
    "    temp_df['shots'] = [20 if row.formatted_prompt_id.split(\"_\")[0] == \"Avg\" else int(row.formatted_prompt_id.split(\"_\")[0]) for _, row in temp_df.iterrows()]\n",
    "\n",
    "    temp_df = temp_df.sort_values(by=['output', 'shots', 'formatted_prompt_id'], ascending=[True, True, True]).reset_index(drop=True)\n",
    "    \n",
    "    temp_df['formatted_prompt_id'] = [row.formatted_prompt_id.replace(\"_output\", \"\")  for _, row in temp_df.iterrows()]\n",
    "    \n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'f1', 'output']]\n",
    "    \n",
    "    column_color = [colours[0] if row.formatted_prompt_id == 'Avg' else (\n",
    "        colours[6] if '_es' in row.formatted_prompt_id else colours[7]) for _, row in temp_df.iterrows()]\n",
    "    \n",
    "    fig = temp_df.plot(x=\"formatted_prompt_id\", y=\"f1\", kind=\"bar\", color=column_color, legend=False)\n",
    "\n",
    "    sec = fig.secondary_xaxis(location=0)\n",
    "    sec.set_xticks([8.5, 25.5], labels=['Output defined', 'Output undefined'])\n",
    "    sec.tick_params('x', length=275, width=0)\n",
    "\n",
    "    generic_chart(f'F1 for NER, grouped by the conditional provision of the output structure', 'Prompts', 'F1 Score') \n",
    "    \n",
    "    print(temp_df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdb5d54a1bc88a9f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223c89184fe8979",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.672402Z",
     "iopub.status.busy": "2024-12-13T11:02:08.672345Z",
     "iopub.status.idle": "2024-12-13T11:02:08.674412Z",
     "shell.execute_reply": "2024-12-13T11:02:08.674220Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1.2, 1.3 F1, precision, recall for NER, RE, NERRE seperated by prompts, add overall average of metrics \n",
    "def generate_evalutation_metrics(task_type):\n",
    "    temp_df = evaluation_df[evaluation_df['task'] == task_type]\n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'precision', 'recall', 'f1']].reset_index(drop=True)\n",
    "    \n",
    "    average_val_row = ['Avg', temp_df['precision'].mean(), temp_df['recall'].mean(), temp_df['f1'].mean()]\n",
    "\n",
    "    temp_df.loc[len(temp_df)] = average_val_row\n",
    "    \n",
    "    temp_df.plot(x=\"formatted_prompt_id\", y=[\"precision\", \"recall\", \"f1\"], kind=\"bar\", color=colours[0:3]) \n",
    "    generic_chart(f'Results for varying the number of shots for {task_type}', 'Prompts', 'Score') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae154835a23641de",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.675495Z",
     "iopub.status.busy": "2024-12-13T11:02:08.675426Z",
     "iopub.status.idle": "2024-12-13T11:02:08.678264Z",
     "shell.execute_reply": "2024-12-13T11:02:08.678059Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Per task --> Hallucinations per prompt stacked with entities extracted\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "# NER based on prompt language\n",
    "def generate_cross_linguistic_stacked_entity_graph(task_type):\n",
    "    temp_ner_hall_df = evaluation_df[evaluation_df['task'] == task_type]\n",
    "\n",
    "    temp_ner_hall_df = temp_ner_hall_df.loc[:, ['formatted_prompt_id', 'extracted_tuples_or_triplets_per_prompt',\n",
    "                                                'tuple_or_triplet_hallucinations_per_prompt', 'language']].sort_values(\n",
    "        by=['language']).reset_index(drop=True)\n",
    "\n",
    "    eng_ner_hall_temp_df = temp_ner_hall_df[temp_ner_hall_df['language'] == 'English prompt']\n",
    "    esp_ner_hall_temp_df = temp_ner_hall_df[temp_ner_hall_df['language'] == 'Spanish prompt']\n",
    "\n",
    "    ner_en_hall_average_val_row = ['Avg_en',\n",
    "                                   math.ceil(eng_ner_hall_temp_df['extracted_tuples_or_triplets_per_prompt'].mean()),\n",
    "                                   math.ceil(eng_ner_hall_temp_df['tuple_or_triplet_hallucinations_per_prompt'].mean()),\n",
    "                                   'English prompt']\n",
    "\n",
    "    temp_ner_hall_df.loc[len(temp_ner_hall_df)] = ner_en_hall_average_val_row\n",
    "\n",
    "    ner_es_hall_average_val_row = ['Avg_es',\n",
    "                                   math.ceil(esp_ner_hall_temp_df['extracted_tuples_or_triplets_per_prompt'].mean()),\n",
    "                                   math.ceil(esp_ner_hall_temp_df['tuple_or_triplet_hallucinations_per_prompt'].mean()),\n",
    "                                   'Spanish prompt']\n",
    "\n",
    "    temp_ner_hall_df.loc[len(temp_ner_hall_df) + 1] = ner_es_hall_average_val_row\n",
    "\n",
    "    temp_ner_hall_df['shots'] = [\n",
    "        20 if row.formatted_prompt_id.split(\"_\")[0] == \"Avg\" else int(row.formatted_prompt_id.split(\"_\")[0]) for _, row\n",
    "        in temp_ner_hall_df.iterrows()]\n",
    "\n",
    "    temp_ner_hall_df = temp_ner_hall_df.sort_values(by=['language', 'shots', 'formatted_prompt_id'],\n",
    "                                                    ascending=[True, True, True]).reset_index(drop=True)\n",
    "\n",
    "    stacked_colours = {\n",
    "        \"extracted_tuples_or_triplets_per_prompt\": [(colours[4] if '_es' in row.formatted_prompt_id else colours[5]) for\n",
    "                                                    _, row in temp_ner_hall_df.iterrows()],\n",
    "        \"tuple_or_triplet_hallucinations_per_prompt\": [(\n",
    "            colours[6] if '_es' in row.formatted_prompt_id else colours[7]) for _, row in temp_ner_hall_df.iterrows()]\n",
    "    }\n",
    "\n",
    "    temp_ner_hall_df['formatted_prompt_id'] = [row.formatted_prompt_id.replace(\"_es\", \"\") for _, row in temp_ner_hall_df.iterrows()]\n",
    "\n",
    "    temp_ner_hall_df['formatted_prompt_id'] = [row.formatted_prompt_id.replace(\"_en\", \"\") for _, row in temp_ner_hall_df.iterrows()]\n",
    "\n",
    "    temp_ner_hall_df = temp_ner_hall_df.loc[:, ['formatted_prompt_id', 'extracted_tuples_or_triplets_per_prompt',\n",
    "                                                'tuple_or_triplet_hallucinations_per_prompt', 'language']]\n",
    "\n",
    "    fig = temp_ner_hall_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", stacked=True, color=stacked_colours)\n",
    "\n",
    "    sec = fig.secondary_xaxis(location=0)\n",
    "    sec.set_xticks([8.5, 25.5], labels=['English prompt (shots)', 'Spanish prompt (shots)'])\n",
    "    sec.tick_params('x', length=245, width=0)\n",
    "\n",
    "    plt.legend(handles=[\n",
    "        mlines.Line2D([], [], color=colours[5], label=\"Extracted instances (en)\", linewidth=10),\n",
    "        mlines.Line2D([], [], color=colours[7], label=\"Hallucinated instances (en)\", linewidth=10),\n",
    "        mlines.Line2D([], [], color=colours[4], label=\"Extracted instances (es)\", linewidth=10),\n",
    "        mlines.Line2D([], [], color=colours[6], label=\"Hallucinated instances (es)\", linewidth=10)], handlelength=0.5)\n",
    "    generic_chart(f'Instances for NER', 'Prompts', 'Instances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19a438aa6e8a92",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.679311Z",
     "iopub.status.busy": "2024-12-13T11:02:08.679248Z",
     "iopub.status.idle": "2024-12-13T11:02:08.681559Z",
     "shell.execute_reply": "2024-12-13T11:02:08.681354Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_stacked_entity_graph(task_type):\n",
    "    temp_df = evaluation_df[evaluation_df['task'] == task_type]\n",
    "    \n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'extracted_tuples_or_triplets_per_prompt', 'tuple_or_triplet_hallucinations_per_prompt']].reset_index(drop=True)\n",
    "    \n",
    "    average_val_row = ['Avg', temp_df['extracted_tuples_or_triplets_per_prompt'].mean(), temp_df['tuple_or_triplet_hallucinations_per_prompt'].mean()]\n",
    "\n",
    "    temp_df.loc[len(temp_df)] = average_val_row\n",
    "    \n",
    "    x1 = temp_df[\"formatted_prompt_id\"].tolist()\n",
    "    y1 = temp_df[\"extracted_tuples_or_triplets_per_prompt\"].tolist()\n",
    "    y2 = temp_df[\"tuple_or_triplet_hallucinations_per_prompt\"].tolist()\n",
    "    \n",
    "    plt.bar(x1, y1, color=colours[4])\n",
    "    plt.bar(x1, y2, bottom=y1, color=colours[5])\n",
    "    \n",
    "    plt.legend([\"Extracted instances\", \"Hallucinated instances\"])\n",
    "    generic_chart(f'Instances for {task_type}', 'Prompts', 'Instances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e6015dcfd7f4",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.682726Z",
     "iopub.status.busy": "2024-12-13T11:02:08.682667Z",
     "iopub.status.idle": "2024-12-13T11:02:08.685779Z",
     "shell.execute_reply": "2024-12-13T11:02:08.685579Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Hallucinations broken down by type of the hallucination (basically looking at the over generation of the found instances and fabrication)\n",
    "# stacked -1 and -2\n",
    "def order_hallucinations_by_type(task_type, df):\n",
    "    offset_col_name = 'offset1'\n",
    "    if task_type == 'RE' or task_type == 'NERRE':\n",
    "        offset_col_name = 'offset1_start'\n",
    "    hallucinations_by_type_df = pd.DataFrame(df.groupby('prompt_id')[offset_col_name].value_counts()).reset_index()\n",
    "\n",
    "    hallucinations_by_type_df_fabrications = hallucinations_by_type_df[\n",
    "        hallucinations_by_type_df[offset_col_name] == -1].reset_index(drop=True).sort_values(by='prompt_id')\n",
    "    hallucinations_by_type_df_fabrications = hallucinations_by_type_df_fabrications.rename(\n",
    "        columns={'count': 'Fabrications'})\n",
    "\n",
    "    hallucinations_by_type_df_over_generated = hallucinations_by_type_df[\n",
    "        hallucinations_by_type_df[offset_col_name] == -2].reset_index(drop=True).sort_values(by='prompt_id')\n",
    "    hallucinations_by_type_df_over_generated = hallucinations_by_type_df_over_generated.rename(\n",
    "        columns={'count': 'Over generated'})\n",
    "\n",
    "    cols = hallucinations_by_type_df_over_generated.columns.difference(hallucinations_by_type_df_fabrications.columns)\n",
    "\n",
    "    refactored_hallucinations_by_type_df = pd.merge(hallucinations_by_type_df_fabrications,\n",
    "                                                    hallucinations_by_type_df_over_generated[cols], left_index=True,\n",
    "                                                    right_index=True, how='outer')\n",
    "\n",
    "    if task_type == 'NER':\n",
    "        refactored_hallucinations_by_type_df = add_language_col(refactored_hallucinations_by_type_df)\n",
    "\n",
    "    refactored_hallucinations_by_type_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in\n",
    "                                                                   refactored_hallucinations_by_type_df.iterrows()]\n",
    "\n",
    "    stacked_colours = colours[6:8]\n",
    "    if task_type == 'NER':\n",
    "        refactored_hallucinations_by_type_df['shots'] = [\n",
    "            20 if row.formatted_prompt_id.split(\"_\")[0] == \"Avg\" else int(row.formatted_prompt_id.split(\"_\")[0]) for\n",
    "            _, row\n",
    "            in refactored_hallucinations_by_type_df.iterrows()]\n",
    "\n",
    "        refactored_hallucinations_by_type_df = refactored_hallucinations_by_type_df.sort_values(\n",
    "            by=['language', 'shots', 'formatted_prompt_id'], ascending=[True, True, True]).reset_index(drop=True)\n",
    "\n",
    "        stacked_colours = {\n",
    "        \"Fabrications\": [(colours[6] if '_es' in row.formatted_prompt_id else colours[7]) for\n",
    "                                                    _, row in refactored_hallucinations_by_type_df.iterrows()],\n",
    "        \"Over generated\": [(\n",
    "            colours[8] if '_es' in row.formatted_prompt_id else colours[9]) for _, row in refactored_hallucinations_by_type_df.iterrows()]\n",
    "        }\n",
    "\n",
    "        refactored_hallucinations_by_type_df['formatted_prompt_id'] = [row.formatted_prompt_id.replace(\"_es\", \"\") for _, row in\n",
    "                                                   refactored_hallucinations_by_type_df.iterrows()]\n",
    "    \n",
    "        refactored_hallucinations_by_type_df['formatted_prompt_id'] = [row.formatted_prompt_id.replace(\"_en\", \"\") for _, row in\n",
    "                                                   refactored_hallucinations_by_type_df.iterrows()]\n",
    "\n",
    "    # refactored_hallucinations_by_type_df = refactored_hallucinations_by_type_df.reset_index(drop=True)\n",
    "\n",
    "    refactored_hallucinations_by_type_df = refactored_hallucinations_by_type_df.loc[:,\n",
    "                                           ['prompt_id', 'formatted_prompt_id', 'Over generated', 'Fabrications']]\n",
    "    fig = refactored_hallucinations_by_type_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", stacked=True,\n",
    "                                                    color=stacked_colours)\n",
    "\n",
    "    if task_type == 'NER':\n",
    "        sec = fig.secondary_xaxis(location=0)\n",
    "        sec.set_xticks([6.5, 21.5], labels=['English prompt (shots)', 'Spanish prompt (shots)'])\n",
    "        sec.tick_params('x', length=245, width=0)\n",
    "        plt.legend(handles=[\n",
    "        mlines.Line2D([], [], color=colours[7], label=\"Fabricated instances (en)\", linewidth=10),\n",
    "        mlines.Line2D([], [], color=colours[9], label=\"Over generated instances (en)\", linewidth=10),\n",
    "        mlines.Line2D([], [], color=colours[6], label=\"Fabricated instances (es)\", linewidth=10),\n",
    "        mlines.Line2D([], [], color=colours[8], label=\"Over generated instances (es)\", linewidth=10)], handlelength=0.5)\n",
    "    else:\n",
    "        plt.legend([\"Over generated instances\", \"Fabricated instances\"])\n",
    "    generic_chart(f'Hallucinations by type for {task_type}', 'Prompts', 'Hallucinated instances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517078b295a4b2f",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.686869Z",
     "iopub.status.busy": "2024-12-13T11:02:08.686801Z",
     "iopub.status.idle": "2024-12-13T11:02:08.689934Z",
     "shell.execute_reply": "2024-12-13T11:02:08.689720Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Hallucinations broken down by the type of the entities and relations\n",
    "def broken_by_entity_type_ner(task_type, df, df_type):\n",
    "    breakdown_by_entity_type_df = pd.DataFrame(df.groupby('prompt_id')['label'].value_counts()).reset_index()\n",
    "\n",
    "    unique_labels = breakdown_by_entity_type_df['label'].unique().tolist()\n",
    "    unique_prompts = breakdown_by_entity_type_df['prompt_id'].unique().tolist()\n",
    "\n",
    "    new_cols = ['prompt_id'] + unique_labels\n",
    "\n",
    "    new_breakdown_by_entity_type_df = pd.DataFrame(columns=new_cols)\n",
    "    for prompt in unique_prompts:\n",
    "        df_row = [prompt]\n",
    "        for label in unique_labels:\n",
    "            # find the count from the df \n",
    "            value = 0\n",
    "            value_tuple = breakdown_by_entity_type_df[\n",
    "                (breakdown_by_entity_type_df['prompt_id'] == prompt) & (breakdown_by_entity_type_df['label'] == label)][\n",
    "                'count']\n",
    "\n",
    "            if not value_tuple.empty:\n",
    "                value = value_tuple.item()\n",
    "\n",
    "            df_row = df_row + [value]\n",
    "\n",
    "        new_breakdown_by_entity_type_df.loc[len(new_breakdown_by_entity_type_df)] = df_row\n",
    "\n",
    "    if task_type == 'NER':\n",
    "        new_breakdown_by_entity_type_df = add_language_col(new_breakdown_by_entity_type_df)\n",
    "\n",
    "    new_breakdown_by_entity_type_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in\n",
    "                                                              new_breakdown_by_entity_type_df.iterrows()]\n",
    "\n",
    "    if task_type == 'NER':\n",
    "        new_breakdown_by_entity_type_df['shots'] = [\n",
    "            20 if row.formatted_prompt_id.split(\"_\")[0] == \"Avg\" else int(row.formatted_prompt_id.split(\"_\")[0]) for\n",
    "            _, row\n",
    "            in new_breakdown_by_entity_type_df.iterrows()]\n",
    "\n",
    "        new_breakdown_by_entity_type_df = new_breakdown_by_entity_type_df.sort_values(\n",
    "            by=['language', 'shots', 'formatted_prompt_id'],\n",
    "            ascending=[True, True, True]).reset_index(drop=True)\n",
    "\n",
    "    new_breakdown_by_entity_type_df = new_breakdown_by_entity_type_df.reset_index(drop=True)\n",
    "\n",
    "    new_cols = ['formatted_prompt_id'] + unique_labels\n",
    "    new_breakdown_by_entity_type_df = new_breakdown_by_entity_type_df.loc[:, new_cols]\n",
    "    fig = new_breakdown_by_entity_type_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", stacked=True, color=colours)\n",
    "\n",
    "    if task_type == 'NER':\n",
    "        sec = fig.secondary_xaxis(location=0)\n",
    "        sec.set_xticks([6.5, 21.5], labels=['English prompt (shots)', 'Spanish prompt (shots)'])\n",
    "        sec.tick_params('x', length=265, width=0)\n",
    "\n",
    "    plt.legend(unique_labels)\n",
    "    generic_chart(f'{df_type} by entity type for {task_type}', 'Prompts', 'Entity count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbea1720ea55a0f",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.691033Z",
     "iopub.status.busy": "2024-12-13T11:02:08.690970Z",
     "iopub.status.idle": "2024-12-13T11:02:08.693990Z",
     "shell.execute_reply": "2024-12-13T11:02:08.693789Z"
    }
   },
   "outputs": [],
   "source": [
    "def broken_by_entity_type_re(task_type, df, df_type):\n",
    "    \n",
    "    unique_labels1 = df['label1'].unique().tolist()\n",
    "    unique_labels2 = df['label2'].unique().tolist()\n",
    "    unique_labels = unique_labels1 + unique_labels2\n",
    "    unique_labels = list(set(unique_labels))\n",
    "    unique_prompts = df['prompt_id'].unique().tolist()\n",
    "    \n",
    "    new_cols = ['prompt_id'] + unique_labels\n",
    "    \n",
    "    label1 = pd.DataFrame(df.groupby('prompt_id')['label1'].value_counts()).reset_index()\n",
    "    label2 = pd.DataFrame(df.groupby('prompt_id')['label2'].value_counts()).reset_index()\n",
    "        \n",
    "    new_hallucinations_by_entity_type_df = pd.DataFrame(columns=new_cols)\n",
    "    for prompt in unique_prompts:\n",
    "        df_row = [prompt]\n",
    "        for label in unique_labels:\n",
    "            # find the count from the df \n",
    "            value = 0\n",
    "            value_tuple_label1 = label1[(label1['prompt_id'] == prompt) & (label1['label1'] == label)]['count']\n",
    "            \n",
    "            value_tuple_label2 = label2[(label2['prompt_id'] == prompt) & (label2['label2'] == label)]['count']\n",
    "                        \n",
    "            if not value_tuple_label1.empty:\n",
    "                value = value_tuple_label1.item()\n",
    "            \n",
    "            if not value_tuple_label2.empty:\n",
    "                value = value + value_tuple_label2.item()\n",
    "                \n",
    "            df_row = df_row + [value]\n",
    "            \n",
    "        new_hallucinations_by_entity_type_df.loc[len(new_hallucinations_by_entity_type_df)] = df_row\n",
    "\n",
    "    new_hallucinations_by_entity_type_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in new_hallucinations_by_entity_type_df.iterrows()]\n",
    "\n",
    "\n",
    "    new_hallucinations_by_entity_type_df = new_hallucinations_by_entity_type_df.sort_values(by='prompt_id').reset_index(drop=True)\n",
    "    \n",
    "    new_cols = ['formatted_prompt_id'] + unique_labels\n",
    "    new_hallucinations_by_entity_type_df = new_hallucinations_by_entity_type_df.loc[:, new_cols]\n",
    "    new_hallucinations_by_entity_type_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", stacked=True, color=colours)\n",
    "        \n",
    "    plt.legend(unique_labels)\n",
    "    generic_chart(f'{df_type} by entity type for {task_type}', 'Prompts', 'Entity count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b3e1d6e4efbd1",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.695096Z",
     "iopub.status.busy": "2024-12-13T11:02:08.695033Z",
     "iopub.status.idle": "2024-12-13T11:02:08.697672Z",
     "shell.execute_reply": "2024-12-13T11:02:08.697462Z"
    }
   },
   "outputs": [],
   "source": [
    "def broken_by_relation_type(task_type, df, df_type):\n",
    "    unique_relations = df['relation_type'].unique().tolist()\n",
    "    unique_prompts = df['prompt_id'].unique().tolist()\n",
    "    \n",
    "    new_cols = ['prompt_id'] + unique_relations\n",
    "    \n",
    "    relations_df = pd.DataFrame(df.groupby('prompt_id')['relation_type'].value_counts(dropna=False)).reset_index()\n",
    "        \n",
    "    broken_by_relation_type_df = pd.DataFrame(columns=new_cols)\n",
    "    for prompt in unique_prompts:\n",
    "        df_row = [prompt]\n",
    "        for relation in unique_relations:\n",
    "            # find the count from the df\n",
    "            value = 0\n",
    "            relation_value = relations_df[(relations_df['prompt_id'] == prompt) & (relations_df['relation_type'] == relation)]['count']\n",
    "                        \n",
    "            if not relation_value.empty:\n",
    "                value = relation_value.item()\n",
    "                \n",
    "            df_row = df_row + [value]\n",
    "            \n",
    "        broken_by_relation_type_df.loc[len(broken_by_relation_type_df)] = df_row\n",
    "    broken_by_relation_type_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in broken_by_relation_type_df.iterrows()]\n",
    "\n",
    "    broken_by_relation_type_df = broken_by_relation_type_df.sort_values(by='prompt_id').reset_index(drop=True)\n",
    "        \n",
    "    new_cols = ['formatted_prompt_id'] + unique_relations\n",
    "    \n",
    "    broken_by_relation_type_df = broken_by_relation_type_df.loc[:, new_cols]\n",
    "    broken_by_relation_type_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", stacked=True, color=colours)\n",
    "        \n",
    "    plt.legend(unique_relations)\n",
    "    generic_chart(f'{df_type} by relation type for {task_type}', 'Prompts', 'Relation count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863341c889aa9721",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.698783Z",
     "iopub.status.busy": "2024-12-13T11:02:08.698724Z",
     "iopub.status.idle": "2024-12-13T11:02:08.701467Z",
     "shell.execute_reply": "2024-12-13T11:02:08.701293Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6. true positives, false negatives, false positives, false positive relations, false negative relations (stacked) vs total extracted entities for each task for exact match vs relaxed match\n",
    "\n",
    "# for ner\n",
    "def cross_linguistic_entity_division(task_type):\n",
    "    entity_division_ner_df = evaluation_df[evaluation_df['task'] == task_type]\n",
    "    entity_division_ner_df = entity_division_ner_df.loc[:, ['formatted_prompt_id', 'true_positive', 'false_positive', 'false_negative', 'language']].sort_values(by=['language']).reset_index(drop=True)\n",
    "    \n",
    "    eng_entity_division_ner_df = entity_division_ner_df[entity_division_ner_df['language'] == 'English prompt']\n",
    "    esp_entity_division_ner_df = entity_division_ner_df[entity_division_ner_df['language'] == 'Spanish prompt']\n",
    "    \n",
    "    average_eng_val_row = ['Avg', eng_entity_division_ner_df['true_positive'].mean(), eng_entity_division_ner_df['false_positive'].mean(), eng_entity_division_ner_df['false_negative'].mean(),'English prompt']\n",
    "    \n",
    "    entity_division_ner_df.loc[len(entity_division_ner_df)] = average_eng_val_row\n",
    "    \n",
    "    average_esp_val_row = ['Avg', esp_entity_division_ner_df['true_positive'].mean(), esp_entity_division_ner_df['false_positive'].mean(), esp_entity_division_ner_df['false_negative'].mean(),'Spanish prompt']\n",
    "    \n",
    "    entity_division_ner_df.loc[len(entity_division_ner_df) + 1] = average_esp_val_row\n",
    "    \n",
    "    entity_division_ner_df['shots'] = [\n",
    "            20 if row.formatted_prompt_id.split(\"_\")[0] == \"Avg\" else int(row.formatted_prompt_id.split(\"_\")[0]) for\n",
    "            _, row\n",
    "            in entity_division_ner_df.iterrows()]\n",
    "\n",
    "    entity_division_ner_df = entity_division_ner_df.sort_values(\n",
    "            by=['language', 'shots', 'formatted_prompt_id'],\n",
    "            ascending=[True, True, True]).reset_index(drop=True)\n",
    "    \n",
    "    fig = entity_division_ner_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", color=colours[5:8])\n",
    "     \n",
    "    sec = fig.secondary_xaxis(location=0)\n",
    "    sec.set_xticks([8.5, 25.5], labels=['English prompts', 'Spanish prompts'])\n",
    "    sec.tick_params('x', length=275, width=0)\n",
    "    \n",
    "    plt.legend(['true_positive', 'false_positive', 'false_negative'])\n",
    "    \n",
    "    generic_chart(f'Entity division for NER grouped by prompt language', 'Prompts', 'Count') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88658024d5cff675",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.702585Z",
     "iopub.status.busy": "2024-12-13T11:02:08.702520Z",
     "iopub.status.idle": "2024-12-13T11:02:08.704702Z",
     "shell.execute_reply": "2024-12-13T11:02:08.704525Z"
    }
   },
   "outputs": [],
   "source": [
    "def entity_division_with_relations(task):\n",
    "    entity_division_df = evaluation_df[evaluation_df['task'] == task]\n",
    "    \n",
    "    entity_division_df = entity_division_df.loc[:, ['formatted_prompt_id', 'true_positive', 'false_positive', 'false_negative', 'false_positive_relations','false_negative_relations']].reset_index(drop=True)\n",
    "    \n",
    "    average_row = ['Avg', entity_division_df['true_positive'].mean(), entity_division_df['false_positive'].mean(), entity_division_df['false_negative'].mean(), \n",
    "     entity_division_df['false_positive_relations'].mean(), entity_division_df['false_negative_relations'].mean()]\n",
    "    \n",
    "    entity_division_df.loc[len(entity_division_df)] = average_row\n",
    "    \n",
    "    entity_division_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", color=colours[5:10])\n",
    "    \n",
    "    plt.legend(['true_positive', 'false_positive', 'false_negative', 'false_positive_relations','false_negative_relations'])\n",
    "    \n",
    "    generic_chart(f'Entity and relation division for {task}', 'Prompts', 'Count') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5a10419c20c83",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.705803Z",
     "iopub.status.busy": "2024-12-13T11:02:08.705739Z",
     "iopub.status.idle": "2024-12-13T11:02:08.707474Z",
     "shell.execute_reply": "2024-12-13T11:02:08.707271Z"
    }
   },
   "outputs": [],
   "source": [
    "def gold_broken_by_entity_type_ner(task_type, df, df_type):\n",
    "    breakdown_by_entity_type = df['label'].value_counts()    \n",
    "    breakdown_by_entity_type.plot(x=\"label\", kind=\"bar\", color=colours)\n",
    "        \n",
    "    generic_chart(f'{df_type} types for {task_type}', 'Type', 'Entity count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0ec9923676c40",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.708504Z",
     "iopub.status.busy": "2024-12-13T11:02:08.708442Z",
     "iopub.status.idle": "2024-12-13T11:02:08.710940Z",
     "shell.execute_reply": "2024-12-13T11:02:08.710745Z"
    }
   },
   "outputs": [],
   "source": [
    "def gold_broken_by_entity_type_re(task_type, df, df_type):\n",
    "    unique_labels1 = df['label1'].unique().tolist()\n",
    "    unique_labels2 = df['label2'].unique().tolist()\n",
    "    unique_labels = unique_labels1 + unique_labels2\n",
    "    unique_labels = list(set(unique_labels))\n",
    "    \n",
    "    new_cols = unique_labels\n",
    "    \n",
    "    label1 = pd.DataFrame(df['label1'].value_counts()).reset_index()\n",
    "    label1.columns = ['label', 'count']\n",
    "    label2 = pd.DataFrame(df['label2'].value_counts()).reset_index()\n",
    "    label2.columns = ['label', 'count']\n",
    "    breakdown_by_entity_type = pd.DataFrame(columns=new_cols)\n",
    "    \n",
    "    df_row = []\n",
    "    for label in unique_labels:\n",
    "        value = 0\n",
    "        value_tuple_label1 = label1[(label1['label'] == label)]['count']\n",
    "        \n",
    "        value_tuple_label2 = label2[(label2['label'] == label)]['count']\n",
    "                    \n",
    "        if not value_tuple_label1.empty:\n",
    "            value = value_tuple_label1.item()\n",
    "        \n",
    "        if not value_tuple_label2.empty:\n",
    "            value = value + value_tuple_label2.item()\n",
    "        df_row = df_row + [value] \n",
    "    breakdown_by_entity_type.loc[len(breakdown_by_entity_type)] = df_row\n",
    "    breakdown_by_entity_type.plot(kind=\"bar\", color=colours)\n",
    "        \n",
    "    generic_chart(f'{df_type} types for {task_type}', 'Entity type', 'Entity count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aae982e1b1a1b5",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.712022Z",
     "iopub.status.busy": "2024-12-13T11:02:08.711963Z",
     "iopub.status.idle": "2024-12-13T11:02:08.713721Z",
     "shell.execute_reply": "2024-12-13T11:02:08.713542Z"
    }
   },
   "outputs": [],
   "source": [
    "def gold_broken_by_relation_type(task_type, df, df_type):\n",
    "    breakdown_by_entity_type = df['relation_type'].value_counts(dropna=False) \n",
    "    \n",
    "    breakdown_by_entity_type.plot(x=\"relation_type\", kind=\"bar\", color=colours)\n",
    "        \n",
    "    generic_chart(f'{df_type} types for {task_type}', 'Type', 'Relation count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.714780Z",
     "iopub.status.busy": "2024-12-13T11:02:08.714707Z",
     "iopub.status.idle": "2024-12-13T11:02:08.720135Z",
     "shell.execute_reply": "2024-12-13T11:02:08.719930Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the files\n",
    "dataset_details = pd.read_csv(f'{result_folder_path}/results/dataset_details/dataset_details.tsv', sep='\\t', header=0)\n",
    "gold_annotation_types = pd.read_csv(f'{result_folder_path}/results/dataset_details/gold_annotation_type_count.tsv', sep='\\t', header=0)\n",
    "evaluation_df = pd.read_csv(f'{result_folder_path}/results/eval_log.tsv', sep='\t', header=0).sort_values(by=['prompt_id'])\n",
    "evaluation_df = add_language_col(evaluation_df)\n",
    "evaluation_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in evaluation_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da3f6d7c8cf8ecb",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.721382Z",
     "iopub.status.busy": "2024-12-13T11:02:08.721316Z",
     "iopub.status.idle": "2024-12-13T11:02:08.731536Z",
     "shell.execute_reply": "2024-12-13T11:02:08.731338Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b510ecdc9f641",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.732707Z",
     "iopub.status.busy": "2024-12-13T11:02:08.732626Z",
     "iopub.status.idle": "2024-12-13T11:02:08.734379Z",
     "shell.execute_reply": "2024-12-13T11:02:08.734215Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = evaluation_df['task'].unique()\n",
    "\n",
    "ner_data_exists = 'NER' in tasks\n",
    "re_data_exists = 'RE' in tasks\n",
    "nerre_data_exists = 'NERRE' in tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce8c67a30c19e",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:08.735382Z",
     "iopub.status.busy": "2024-12-13T11:02:08.735314Z",
     "iopub.status.idle": "2024-12-13T11:02:11.001502Z",
     "shell.execute_reply": "2024-12-13T11:02:11.001209Z"
    }
   },
   "outputs": [],
   "source": [
    "if ner_data_exists:\n",
    "    ner_extracted_entities = pd.read_csv(f'{result_folder_path}/results/entities/NER/results.tsv', sep='\\t', header=0)\n",
    "    ner_gold_entities = pd.read_csv(f'{result_folder_path}/results/entities/NER/gold.tsv', sep='\\t', header=0)\n",
    "    \n",
    "    ner_hallucinations_df = merge_hallucinations('NER')\n",
    "    ner_hallucinations_df = add_language_col(ner_hallucinations_df)\n",
    "    ner_hallucinations_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in ner_hallucinations_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5349227608c54098",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:11.002831Z",
     "iopub.status.busy": "2024-12-13T11:02:11.002763Z",
     "iopub.status.idle": "2024-12-13T11:02:11.004948Z",
     "shell.execute_reply": "2024-12-13T11:02:11.004749Z"
    }
   },
   "outputs": [],
   "source": [
    "if re_data_exists:\n",
    "    re_extracted_entities = pd.read_csv(f'{result_folder_path}/results/entities/RE/results.tsv', sep='\\t', header=0)\n",
    "    re_extracted_entities = re_extracted_entities.fillna('NA')\n",
    "    \n",
    "    re_gold_entities = pd.read_csv(f'{result_folder_path}/results/entities/RE/gold.tsv', sep='\\t', header=0)\n",
    "    re_gold_entities = re_gold_entities.fillna('NA')\n",
    "    \n",
    "    re_hallucinations_df = merge_hallucinations('RE')\n",
    "    re_hallucinations_df = re_hallucinations_df.fillna('NA')\n",
    "    re_hallucinations_df = add_language_col(re_hallucinations_df)\n",
    "    re_hallucinations_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in re_hallucinations_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50849e289a4dd",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:11.006082Z",
     "iopub.status.busy": "2024-12-13T11:02:11.006012Z",
     "iopub.status.idle": "2024-12-13T11:02:11.008232Z",
     "shell.execute_reply": "2024-12-13T11:02:11.008037Z"
    }
   },
   "outputs": [],
   "source": [
    "if nerre_data_exists:\n",
    "    nerre_extracted_entities = pd.read_csv(f'{result_folder_path}/results/entities/NERRE/results.tsv', sep='\\t', header=0)\n",
    "    nerre_extracted_entities = nerre_extracted_entities.fillna('NA')\n",
    "    nerre_extracted_entities['prompt_id'] = nerre_extracted_entities['prompt_id'].replace({'p1_one_shot': 'p2_one_shot'})\n",
    "    \n",
    "    nerre_gold_entities = pd.read_csv(f'{result_folder_path}/results/entities/NERRE/gold.tsv', sep='\\t', header=0)\n",
    "    nerre_gold_entities = nerre_gold_entities.fillna('NA')\n",
    "    \n",
    "    nerre_hallucinations_df = merge_hallucinations('NERRE')\n",
    "    nerre_hallucinations_df = nerre_hallucinations_df.fillna('NA')\n",
    "    nerre_hallucinations_df = add_language_col(nerre_hallucinations_df)\n",
    "    nerre_hallucinations_df['prompt_id'] = nerre_hallucinations_df['prompt_id'].replace({'p1_one_shot': 'p2_one_shot'})\n",
    "    nerre_hallucinations_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in nerre_hallucinations_df.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f799180b1f9b718",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Performance of the gen LLM based on prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99efa9e9aa7232d",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:11.009418Z",
     "iopub.status.busy": "2024-12-13T11:02:11.009344Z",
     "iopub.status.idle": "2024-12-13T11:02:11.551153Z",
     "shell.execute_reply": "2024-12-13T11:02:11.550913Z"
    }
   },
   "outputs": [],
   "source": [
    "if ner_data_exists: \n",
    "    generate_cross_linguistic_evaluation_metrics('NER')\n",
    "    generate_cross_linguistic_f1_shots('NER')\n",
    "    generate_cross_linguistic_f1_guideline('NER')\n",
    "    generate_cross_linguistic_f1_output('NER')\n",
    "    \n",
    "if re_data_exists: generate_evalutation_metrics('RE')\n",
    "if nerre_data_exists: generate_evalutation_metrics('NERRE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0e5fab434ae52",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Exploration of the hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e46f39e6a9e02",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:11.552420Z",
     "iopub.status.busy": "2024-12-13T11:02:11.552345Z",
     "iopub.status.idle": "2024-12-13T11:02:12.867082Z",
     "shell.execute_reply": "2024-12-13T11:02:12.866793Z"
    }
   },
   "outputs": [],
   "source": [
    "if ner_data_exists:\n",
    "    generate_cross_linguistic_stacked_entity_graph('NER')\n",
    "    order_hallucinations_by_type('NER', ner_hallucinations_df)\n",
    "    broken_by_entity_type_ner('NER', ner_hallucinations_df, 'Hallucinations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42bc5f136a34c6",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:12.868609Z",
     "iopub.status.busy": "2024-12-13T11:02:12.868460Z",
     "iopub.status.idle": "2024-12-13T11:02:12.870295Z",
     "shell.execute_reply": "2024-12-13T11:02:12.870069Z"
    }
   },
   "outputs": [],
   "source": [
    "if re_data_exists:\n",
    "    generate_stacked_entity_graph('RE')\n",
    "    order_hallucinations_by_type('RE', re_hallucinations_df)\n",
    "    broken_by_entity_type_re('RE', re_hallucinations_df, 'Hallucinations')\n",
    "    broken_by_relation_type('RE', re_hallucinations_df, 'Hallucinations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306102e9e7c22dff",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:12.871641Z",
     "iopub.status.busy": "2024-12-13T11:02:12.871559Z",
     "iopub.status.idle": "2024-12-13T11:02:12.873160Z",
     "shell.execute_reply": "2024-12-13T11:02:12.872932Z"
    }
   },
   "outputs": [],
   "source": [
    "if nerre_data_exists:\n",
    "    generate_stacked_entity_graph('NERRE')\n",
    "    order_hallucinations_by_type('NERRE', nerre_hallucinations_df)\n",
    "    broken_by_relation_type('NERRE', nerre_hallucinations_df, 'Hallucinations')\n",
    "    broken_by_entity_type_re('NERRE', nerre_hallucinations_df, 'Hallucinations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11016f6ebbbbb1b1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Classification of the extracted entities and identified relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df85ba4b178545",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:12.874316Z",
     "iopub.status.busy": "2024-12-13T11:02:12.874249Z",
     "iopub.status.idle": "2024-12-13T11:02:14.124228Z",
     "shell.execute_reply": "2024-12-13T11:02:14.123887Z"
    }
   },
   "outputs": [],
   "source": [
    "# 7. Breakdown of the gold entities and relations vs extracted entities and relations for each task\n",
    "if ner_data_exists:\n",
    "    cross_linguistic_entity_division('NER')\n",
    "    \n",
    "    broken_by_entity_type_ner('NER', ner_extracted_entities, 'Extracted entities')\n",
    "    gold_broken_by_entity_type_ner('NER', ner_gold_entities, 'Gold entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d58cb61375432",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:14.125609Z",
     "iopub.status.busy": "2024-12-13T11:02:14.125512Z",
     "iopub.status.idle": "2024-12-13T11:02:14.127562Z",
     "shell.execute_reply": "2024-12-13T11:02:14.127327Z"
    }
   },
   "outputs": [],
   "source": [
    "if re_data_exists:\n",
    "    entity_division_with_relations('RE')\n",
    "    \n",
    "    broken_by_relation_type('RE', re_extracted_entities, 'Extracted entities')\n",
    "    broken_by_entity_type_re('RE', re_extracted_entities, 'Extracted entities')\n",
    "    \n",
    "    gold_broken_by_entity_type_re('RE', re_gold_entities, 'Gold entity')\n",
    "    gold_broken_by_relation_type('RE', re_gold_entities, 'Gold relation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e9c9d50794b6a",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:14.128710Z",
     "iopub.status.busy": "2024-12-13T11:02:14.128631Z",
     "iopub.status.idle": "2024-12-13T11:02:14.130508Z",
     "shell.execute_reply": "2024-12-13T11:02:14.130311Z"
    }
   },
   "outputs": [],
   "source": [
    "if nerre_data_exists:\n",
    "    entity_division_with_relations('NERRE')\n",
    "    \n",
    "    broken_by_relation_type('NERRE', nerre_extracted_entities, 'Extracted entities')\n",
    "    broken_by_entity_type_re('NERRE', nerre_extracted_entities, 'Extracted entities')\n",
    "    \n",
    "    gold_broken_by_entity_type_re('NERRE', nerre_gold_entities, 'Gold entity')\n",
    "    gold_broken_by_relation_type('NERRE', nerre_gold_entities, 'Gold relation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
