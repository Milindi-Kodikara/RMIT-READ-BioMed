{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 10)\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "opal = '#275c4d'\n",
    "ruby = '#af221d'\n",
    "topaz = '#c59103'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d617edcb4643e45",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generic_chart(title, x_label, y_label):\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.xticks()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5409d5107d7986ed",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# read the files\n",
    "dataset_details = pd.read_csv('./exact_results/dataset_details/dataset_details.tsv', sep='\\t', header=0)\n",
    "gold_annotation_types = pd.read_csv('./exact_results/dataset_details/gold_annotation_type_count.tsv', sep='\\t', header=0)\n",
    "evaluation_df = pd.read_csv('./exact_results/eval_log.tsv', sep='\t', header=0).sort_values(by=['prompt_id'])\n",
    "\n",
    "def merge_hallucinations(task):\n",
    "    tsv_files = glob.glob(f'./exact_results/hallucinations/{task}/*.tsv')\n",
    "    \n",
    "    combined_df = pd.DataFrame()\n",
    "    for tsv_file in tsv_files:\n",
    "        df = pd.read_csv(tsv_file, sep='\\t', header=0)\n",
    "        combined_df = pd.concat([combined_df, df])\n",
    "        \n",
    "    return combined_df\n",
    "\n",
    "ner_hallucinations_df = merge_hallucinations('NER')\n",
    "re_hallucinations_df = merge_hallucinations('RE')\n",
    "nerre_hallucinations_df = merge_hallucinations('NERRE')"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Add new column for the language \n",
    "def add_language_col(df):\n",
    "    df['language'] = [\"Spanish prompt\" if '_es'in row.prompt_id  else 'English prompt' for _, row in df.iterrows()]\n",
    "    \n",
    "    return df\n",
    "\n",
    "evaluation_df = add_language_col(evaluation_df)\n",
    "ner_hallucinations_df = add_language_col(ner_hallucinations_df)\n",
    "re_hallucinations_df = add_language_col(re_hallucinations_df)\n",
    "nerre_hallucinations_df = add_language_col(nerre_hallucinations_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8cb2589ed6bd274",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Add formatted prompt id\n",
    "def add_prompt_name(prompt_id):\n",
    "    if \"zero\" in prompt_id:\n",
    "        return \"Zero shot\"\n",
    "    elif \"one\" in prompt_id:\n",
    "        return \"One shot\"\n",
    "    elif \"five\" in prompt_id:\n",
    "        return \"Five shot\"\n",
    "    elif \"ten\" in prompt_id:\n",
    "        return \"Ten shot\"\n",
    "    else: \n",
    "        return prompt_id\n",
    "\n",
    "evaluation_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in evaluation_df.iterrows()]\n",
    "\n",
    "ner_hallucinations_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in ner_hallucinations_df.iterrows()]\n",
    "\n",
    "re_hallucinations_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in re_hallucinations_df.iterrows()]\n",
    "\n",
    "nerre_hallucinations_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in nerre_hallucinations_df.iterrows()]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5349227608c54098",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Performance of the gen LLM based on prompts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f799180b1f9b718"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1.1/2.0 NER performance against different prompts grouped by language \n",
    "temp_df = evaluation_df[evaluation_df['task'] == 'NER']\n",
    "temp_df = temp_df\n",
    "temp_df = temp_df.loc[:, ['formatted_prompt_id', 'precision', 'recall', 'f1', 'language']].sort_values(by=['language']).reset_index(drop=True)\n",
    "\n",
    "eng_temp_df = temp_df[temp_df['language'] == 'English prompt']\n",
    "esp_temp_df = temp_df[temp_df['language'] == 'Spanish prompt']\n",
    "\n",
    "average_eng_val_row = ['Average', eng_temp_df['precision'].mean(), eng_temp_df['recall'].mean(), eng_temp_df['f1'].mean(), 'English prompt']\n",
    "\n",
    "temp_df.loc[len(temp_df)] = average_eng_val_row\n",
    "\n",
    "average_esp_val_row = ['Average', esp_temp_df['precision'].mean(), esp_temp_df['recall'].mean(), esp_temp_df['f1'].mean(), 'Spanish prompt']\n",
    "\n",
    "temp_df.loc[len(temp_df) + 1] = average_esp_val_row\n",
    "\n",
    "temp_df = temp_df.sort_values(by=['language']).reset_index(drop=True)\n",
    "\n",
    "fig = temp_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", color=[opal, topaz, ruby])\n",
    " \n",
    "sec = fig.secondary_xaxis(location=0)\n",
    "sec.set_xticks([2, 6], labels=['English prompts', 'Spanish prompts'])\n",
    "sec.tick_params('x', length=80, width=0)\n",
    "\n",
    "generic_chart(f'Evaluation metrics for NER grouped by prompt language', 'Prompts', 'Score') "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c677fbc8ce2a80f4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1.2, 1.3 F1, precision, recall for NER, RE, NERRE seperated by prompts, add overall average of metrics \n",
    "\n",
    "def generate_evalutation_metrics(task_type):\n",
    "    temp_df = evaluation_df[evaluation_df['task'] == task_type]\n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'precision', 'recall', 'f1']].reset_index(drop=True)\n",
    "    \n",
    "    average_val_row = ['Average', temp_df['precision'].mean(), temp_df['recall'].mean(), temp_df['f1'].mean()]\n",
    "\n",
    "    temp_df.loc[len(temp_df)] = average_val_row\n",
    "    \n",
    "    temp_df.plot(x=\"formatted_prompt_id\", y=[\"precision\", \"recall\", \"f1\"], kind=\"bar\", color=[opal, topaz, ruby]) \n",
    "    generic_chart(f'Evaluation metrics for {task_type}', 'Prompts', 'Score') \n",
    "\n",
    "generate_evalutation_metrics('RE')\n",
    "generate_evalutation_metrics('NERRE')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a99efa9e9aa7232d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exploration of the hallucinations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23a0e5fab434ae52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 3. Per task --> Hallucinations per prompt stacked with entities extracted\n",
    "\n",
    "# NER based on prompt language\n",
    "temp_ner_hall_df = evaluation_df[evaluation_df['task'] == 'NER']\n",
    "\n",
    "temp_ner_hall_df = temp_ner_hall_df.loc[:, ['formatted_prompt_id', 'extracted_tuples_or_triplets_per_prompt', 'tuple_or_triplet_hallucinations_per_prompt', 'language']].sort_values(by=['language']).reset_index(drop=True)\n",
    "\n",
    "eng_ner_hall_temp_df = temp_ner_hall_df[temp_ner_hall_df['language'] == 'English prompt']\n",
    "esp_ner_hall_temp_df = temp_ner_hall_df[temp_ner_hall_df['language'] == 'Spanish prompt']\n",
    "\n",
    "ner_en_hall_average_val_row = ['Average', math.ceil(eng_ner_hall_temp_df['extracted_tuples_or_triplets_per_prompt'].mean()), math.ceil(eng_ner_hall_temp_df ['tuple_or_triplet_hallucinations_per_prompt'].mean()), 'English prompt']\n",
    "\n",
    "temp_ner_hall_df.loc[len(temp_ner_hall_df)] = ner_en_hall_average_val_row\n",
    "\n",
    "ner_es_hall_average_val_row = ['Average', math.ceil(esp_ner_hall_temp_df['extracted_tuples_or_triplets_per_prompt'].mean()), math.ceil(esp_ner_hall_temp_df ['tuple_or_triplet_hallucinations_per_prompt'].mean()), 'Spanish prompt']\n",
    "\n",
    "temp_ner_hall_df.loc[len(temp_ner_hall_df) + 1] = ner_es_hall_average_val_row\n",
    "\n",
    "temp_ner_hall_df = temp_ner_hall_df.sort_values(by=['language']).reset_index(drop=True)\n",
    "\n",
    "fig = temp_ner_hall_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", stacked=True, color=[ruby, topaz])\n",
    " \n",
    "sec = fig.secondary_xaxis(location=0)\n",
    "sec.set_xticks([2, 6], labels=['English prompts', 'Spanish prompts'])\n",
    "sec.tick_params('x', length=80, width=0)\n",
    "\n",
    "plt.legend([\"Extracted instances\", \"Hallucinated instances\"])\n",
    "generic_chart(f'Instances for NER', 'Prompts', 'Instances')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae154835a23641de",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_stacked_entity_graph(task_type):\n",
    "    temp_df = evaluation_df[evaluation_df['task'] == task_type]\n",
    "    \n",
    "    temp_df = temp_df.loc[:, ['formatted_prompt_id', 'extracted_tuples_or_triplets_per_prompt', 'tuple_or_triplet_hallucinations_per_prompt']].reset_index(drop=True)\n",
    "    \n",
    "    average_val_row = ['Average', temp_df['extracted_tuples_or_triplets_per_prompt'].mean(), temp_df['tuple_or_triplet_hallucinations_per_prompt'].mean()]\n",
    "\n",
    "    temp_df.loc[len(temp_df)] = average_val_row\n",
    "    \n",
    "    x1 = temp_df[\"formatted_prompt_id\"].tolist()\n",
    "    y1 = temp_df[\"extracted_tuples_or_triplets_per_prompt\"].tolist()\n",
    "    y2 = temp_df[\"tuple_or_triplet_hallucinations_per_prompt\"].tolist()\n",
    "    \n",
    "    plt.bar(x1, y1, color=ruby)\n",
    "    plt.bar(x1, y2, bottom=y1, color=topaz)\n",
    "    \n",
    "    plt.legend([\"Extracted instances\", \"Hallucinated instances\"])\n",
    "    generic_chart(f'Instances for {task_type}', 'Prompts', 'Instances')\n",
    "\n",
    "# TODO: Add a note specifying that NER shows the span-label tuple instances while RE and NERRE shows spans-labels-relation triplet instances\n",
    "generate_stacked_entity_graph('RE')\n",
    "generate_stacked_entity_graph('NERRE')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c82e46f39e6a9e02",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 4. Hallucinations broken down by type of the hallucination (basically looking at the over generation of the found instances and fabrication)\n",
    "\n",
    "# stacked -1 and -2\n",
    "def order_hallucinations_by_type(task_type, df):\n",
    "    offset_col_name = 'offset1'\n",
    "    if task_type == 'RE' or task_type == 'NERRE':\n",
    "        offset_col_name = 'offset1_start'\n",
    "    hallucinations_by_type_df = pd.DataFrame(df.groupby('prompt_id')[offset_col_name].value_counts()).reset_index()\n",
    "    \n",
    "    hallucinations_by_type_df_fabrications = hallucinations_by_type_df[hallucinations_by_type_df[offset_col_name] == -1].reset_index(drop=True).sort_values(by='prompt_id')\n",
    "    hallucinations_by_type_df_fabrications = hallucinations_by_type_df_fabrications.rename(columns={'count': 'Fabrications'})\n",
    "    \n",
    "    hallucinations_by_type_df_over_generated = hallucinations_by_type_df[hallucinations_by_type_df[offset_col_name] == -2].reset_index(drop=True).sort_values(by='prompt_id')\n",
    "    hallucinations_by_type_df_over_generated = hallucinations_by_type_df_over_generated.rename(columns={'count': 'Over generated'})\n",
    "    \n",
    "    cols = hallucinations_by_type_df_over_generated.columns.difference(hallucinations_by_type_df_fabrications.columns)\n",
    "    \n",
    "    refactored_ner_hallucinations_by_type_df = pd.merge(hallucinations_by_type_df_fabrications, hallucinations_by_type_df_over_generated[cols], left_index=True, right_index=True, how='outer')\n",
    "    \n",
    "    if task_type == 'NER':\n",
    "        refactored_ner_hallucinations_by_type_df = add_language_col(refactored_ner_hallucinations_by_type_df)\n",
    "    \n",
    "    refactored_ner_hallucinations_by_type_df['formatted_prompt_id'] = [add_prompt_name(row.prompt_id) for _, row in refactored_ner_hallucinations_by_type_df.iterrows()]\n",
    "    \n",
    "    if task_type == 'NER':\n",
    "        refactored_ner_hallucinations_by_type_df = refactored_ner_hallucinations_by_type_df.sort_values(by='language')\n",
    "    \n",
    "    refactored_ner_hallucinations_by_type_df = refactored_ner_hallucinations_by_type_df.reset_index(drop=True)\n",
    "    \n",
    "    refactored_ner_hallucinations_by_type_df = refactored_ner_hallucinations_by_type_df.loc[:, ['prompt_id', 'formatted_prompt_id', 'Over generated', 'Fabrications']]\n",
    "    fig = refactored_ner_hallucinations_by_type_df.plot(x=\"formatted_prompt_id\", kind=\"bar\", stacked=True, color=[ruby, topaz])\n",
    "    \n",
    "    if task_type == 'NER': \n",
    "        sec = fig.secondary_xaxis(location=0)\n",
    "        sec.set_xticks([2, 6], labels=['English prompts', 'Spanish prompts'])\n",
    "        sec.tick_params('x', length=80, width=0)\n",
    "        \n",
    "    plt.legend([\"Over generated instances\", \"Fabricated instances\"])\n",
    "    generic_chart(f'Hallucinations by type for {task_type}', 'Prompts', 'Hallucinated instances')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "325e6015dcfd7f4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "order_hallucinations_by_type('NER', ner_hallucinations_df)\n",
    "order_hallucinations_by_type('RE', re_hallucinations_df)\n",
    "order_hallucinations_by_type('NERRE', nerre_hallucinations_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac42bc5f136a34c6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: 5. Hallucinations broken down by the type of the entities and relations\n",
    "hallucinations_by_label_type_df = pd.DataFrame(ner_hallucinations_df.groupby('prompt_id')['label'].value_counts()).reset_index()\n",
    "\n",
    "hallucinations_by_label_type_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a588fdd8cf856fc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ner_hallucinations_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "306102e9e7c22dff",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Classification of the extracted entities and identified relations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11016f6ebbbbb1b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: 6. true positives, false negatives, false positives, false positive relations, false negative relations (stacked) vs total extracted entities for each task for exact match vs relaxed match"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "863341c889aa9721",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: 7. Breakdown of the gold entities and relations vs extracted entities and relations for each task"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18df85ba4b178545",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
