{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0881c944-4aed-4da2-8e21-72412033c1ee",
   "metadata": {},
   "source": [
    "# NER using GPT-3.5\n",
    "\n",
    "### Project name: Honos\n",
    "Date: 24th May 2024\n",
    "\n",
    "Author: Milindi Kodikara | Supervisor: Professor Karin Verspoor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0337bbac-21c4-45c9-b0ca-922bfbc97b72",
   "metadata": {},
   "source": [
    "\n",
    "Before running this notebook:\n",
    "1. [Install Jupyter notebook](https://jupyter.org/install) \n",
    "\n",
    "\n",
    "2. [Setting up Azure OpenAI model](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/working-with-models?tabs=powershell#model-updates)\n",
    "\n",
    "\n",
    "3. [Setting up connection to GPT-3.5 using Azure OpenAI service](https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-python)\n",
    "        - In the Environment variables section, instead of doing what is outlined in the link, add the `API_KEY`, `API-VERSION`, `ENDPOINT` and `DEPLOYMENT-NAME` into a `.env` file in the root folder.\n",
    "        \n",
    "4. Add the correct filename paths for `data` in Step 1 and gold annotated data filename for the `evaluate()` function in Step 4. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f739ac-16fa-4a1b-9855-e8b89fdeb194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d926f35-0054-4d89-bfda-b3d3d589dcc5",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Load and pre-process data and prompt library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a7d5cb-7d77-449d-a3f1-bcfb1281883d",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 1.1: Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0a4e4-b5b7-441f-b99f-29e18f7ed302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_text.tsv\n",
    "# pmid\\tfilename\\ttext\n",
    "\n",
    "# TODO: Replace filepath for related data file, Tom comment out this whole cell\n",
    "original_data = pd.read_csv(\"../data/ner/genovardis_train_dev_test/test_text.tsv\", sep='\\t', header=0)\n",
    "\n",
    "# this type will be appended to the final result file\n",
    "data_type = 'test'\n",
    "\n",
    "# If output needed in the BRAT format\n",
    "generate_brat_format = False\n",
    "\n",
    "# gold ann file to be bratified\n",
    "gold_annotation_filepath = '../data/ner/genovardis_train_dev_test/dev_annotation.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "original_data.head(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25ca92b2075915ca",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(original_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c170e2b03454265",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: remove this after testing\n",
    "# original_data = original_data.head(2)\n",
    "# \n",
    "# original_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be60d3202a45483d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# original data has the pmid instances in the text\n",
    "data = original_data.copy(deep=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0c0a8b4ea22dfc9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "original_data.sample()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1430a879bcdba3ec",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# clean up text by removing the appended pmid and title abstract tags at the start of each section\n",
    "\n",
    "pattern = '(?:[\\d]{1,10}\\|t\\|)(?P<title>[\\w\\W]+)(?:\\\\n[\\d]{1,20}\\|a\\|)(?P<abstract>[\\w\\W]+)'\n",
    "\n",
    "def clean_text(text):\n",
    "    matches = re.search(pattern, text)\n",
    "    reformatted_text = f'{matches.group(\"title\")}\\n{matches.group(\"abstract\")}'\n",
    "    return reformatted_text\n",
    "\n",
    "data['text'] = [clean_text(text) for text in data['text']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d96ff234e2653dd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data.sample()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db9931374105502c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6296d4e5d31985f9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Step 1.2: Load prompt library\n",
    "\n",
    "Prompt id structure:\n",
    "`p_<index>_<task>_<language>_<output>`\n",
    "\n",
    "TODO: Figure out `<guideline>_<paradigm>`"
   ],
   "metadata": {},
   "id": "da74db47-221a-40f1-9e02-2fd68220b367"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt_library = pd.read_json('prompts.json')\n",
    "\n",
    "prompt_library"
   ],
   "metadata": {},
   "id": "fb1d18f8-110f-4981-bc3a-40c417cb490a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: remove this after testing, checking each prompt using the index\n",
    "prompt_library = prompt_library.loc[[7]]\n",
    "\n",
    "prompt_library"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f999fda207f1d6d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Step 1.3: Create data+prompt dataset"
   ],
   "metadata": {},
   "id": "7e17606d-7a57-4044-94b3-ef4b802af0f4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load in 10 examples from training data\n",
    "text_training_data = pd.read_csv(\"../data/ner/genovardis_train_dev_test/train_text.tsv\", sep='\\t', header=0)\n",
    "annotated_training_data = pd.read_csv('../data/ner/genovardis_train_dev_test/train_annotation.tsv', sep='\\t', header=0)\n",
    "\n",
    "text_training_data['text'] = [clean_text(text) for text in text_training_data['text']]\n",
    "\n",
    "print(f\"Text data len: {len(text_training_data)}, Ann len: {len(annotated_training_data)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1af12f13a2c274b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text_training_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc9e33e926722b55",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "annotated_training_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26cf0cc62f443c76",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# combine label, span into string\n",
    "annotated_training_data['label-span'] = [f\"{row['label']}\\t{row['span']}\" for _, row in annotated_training_data.iterrows()]\n",
    "\n",
    "annotated_training_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27447213a6620c97",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "split_annotated_training_data = annotated_training_data.loc[:, ['pmid', 'label-span']]\n",
    "\n",
    "len(split_annotated_training_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f05b0d02c0e8cd0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "split_annotated_training_data = split_annotated_training_data.groupby('pmid')['label-span'].apply('\\n'.join)\n",
    "\n",
    "len(split_annotated_training_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ea52527880592f4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "split_annotated_training_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64616b77610f1eb0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "merged_training_data = pd.merge(text_training_data, split_annotated_training_data, on=\"pmid\")\n",
    "merged_training_data['text-label-span'] = [f\"Given an example text \\\"{row['text']}\\\", the output is: \\\"\\nlabel\\tspan\\n{row['label-span']}\\\"\" for _, row in merged_training_data.iterrows()]\n",
    "\n",
    "examples_df = merged_training_data.loc[:, ['pmid', 'text-label-span']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9d2e508f631701e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "examples_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28c0e1df5dd9d2e9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "examples_df.iloc[0]['text-label-span']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81e796478ede920",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# pmid prompt_id embedded_prompt\n",
    "def embed_data_in_prompts(row_data):\n",
    "    prompts = []\n",
    "    pmid = row_data['pmid']\n",
    "    data_text = row_data['text']\n",
    "    \n",
    "    for index, row_prompt in prompt_library.iterrows():\n",
    "        instruction = row_prompt['instruction']\n",
    "        guideline = row_prompt['guideline']\n",
    "        no_of_examples = row_prompt['examples']\n",
    "        examples = '\\n'.join(examples_df.iloc[: no_of_examples]['text-label-span'])\n",
    "        expected_output = row_prompt['expected_output']\n",
    "        prompt_text = row_prompt['text'].format(data_text)\n",
    "        \n",
    "        prompt_structure = [guideline, examples, instruction, expected_output, prompt_text]\n",
    "        concatenated_prompt = '\\n\\n'.join(prompt_structure)\n",
    "        \n",
    "        \n",
    "        prompt = {'prompt_id': row_prompt['prompt_id'], 'prompt': concatenated_prompt}\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    return {'pmid': pmid, 'prompts': prompts}\n"
   ],
   "metadata": {},
   "id": "6dc0bb8a-f112-432c-aee1-335dbf175970",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embedded_prompt_data_list = [embed_data_in_prompts(row_data) for index, row_data in data.iterrows()]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e3fd79116cffa1c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(embedded_prompt_data_list[0]['prompts'][0]['prompt'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5905c131cf4c8e7b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eac6e4d1-bfb4-4c90-8b7b-d0ffc71d6313",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Setting up GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac339d-8467-4651-bc3f-72faa835033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.environ[\"API-KEY\"],  \n",
    "    api_version=os.environ[\"API-VERSION\"],\n",
    "    azure_endpoint=os.environ[\"ENDPOINT\"]\n",
    "    )\n",
    "    \n",
    "deployment_name=os.environ[\"DEPLOYMENT-NAME\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Testing the connection\n",
    "test_response = client.chat.completions.create(model=deployment_name, messages=[{\"role\": \"user\", \"content\": \"Hello, World!\"}])\n",
    "print(test_response.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efd39ba90dcb4d0b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: Ask Karin whether we should run again and again to see what gpt generates - yes! later!\n",
    "results_list = []\n",
    "def generate_results(prompt_items):\n",
    "    \n",
    "    pmid = prompt_items['pmid']\n",
    "    \n",
    "    for prompt_item in prompt_items['prompts']:\n",
    "    \n",
    "        prompt_id = prompt_item['prompt_id']\n",
    "        prompt = prompt_item['prompt']\n",
    "        \n",
    "        # TODO: Look into hyper params like temp \n",
    "        response = client.chat.completions.create(model=deployment_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        \n",
    "        response_result = response.choices[0].message.content\n",
    "        \n",
    "        results_list.append({'pmid': pmid, 'prompt_id': prompt_id, 'result': response_result})\n",
    "    \n",
    "        # print(f'Prompt:\\n{prompt}\\n\\nResponse:\\n{response_result} \\n----------\\n')\n",
    "        print(f'Prompt_id:\\n{prompt_id}\\n\\npmid:\\n{pmid}\\n----------\\n')\n",
    "    \n",
    "    return results_list\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b71f8c1697ad95c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for embedded_prompt_data in embedded_prompt_data_list:\n",
    "    generate_results(embedded_prompt_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ff8b7c3321ce93d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac9d89026d543e42",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(results_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f90944f4d4f74661",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0ce5f688-6a65-4e58-a070-22caf6cdf95d",
   "metadata": {},
   "source": [
    "### Step 3: Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# create df from results list and data df\n",
    "# columns = pmid, prompt_id, filename, label, offset1, offset2, span\n",
    "extracted_entity_results = pd.DataFrame(columns=['pmid','prompt_id','filename','label', 'offset_checked', 'offset1','offset2','span'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81729923a951ac42",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(extracted_entity_results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b445e9eccce4b77",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "label_entity_pattern = '^(?P<label>DNAMutation|SNP|DNAAllele|NucleotideChange-BaseChange|OtherMutation|Gene|Disease|Transcript)\\s+(?P<span>[\\w\\W]+)$'\n",
    "\n",
    "def extract_tuple(tuple_string):\n",
    "    stripped_tuple_string = tuple_string.strip()\n",
    "    matches = re.search(label_entity_pattern, stripped_tuple_string)\n",
    "    \n",
    "    if not matches:\n",
    "        return\n",
    "    \n",
    "    label = matches.group(\"label\").strip()\n",
    "    span = matches.group(\"span\").strip()\n",
    "    \n",
    "    return {'label': label, 'span': span}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ad88bf60dafa5b0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# extract each entity from the combined result string from gpt-3.5\n",
    "# add each extracted tuple as a new row in extracted_entity_results df\n",
    "def extract_ner_results(pmid, prompt_id, result_string):\n",
    "    if result_string:\n",
    "        extracted_list = result_string.splitlines()\n",
    "        extracted_tuple_list = [ extract_tuple(result_string) for result_string in extracted_list]\n",
    "        \n",
    "        for extracted_tuple in extracted_tuple_list:\n",
    "            if extracted_tuple:\n",
    "                filename = data.loc[data['pmid'] == pmid, 'filename'].iloc[0]\n",
    "                filename_ann = filename.replace('txt', 'ann')\n",
    "                df_row = {\n",
    "                        \"pmid\": pmid,\n",
    "                        \"prompt_id\": prompt_id,\n",
    "                        \"filename\" : filename_ann,\n",
    "                        \"label\": extracted_tuple['label'],\n",
    "                        \"offset_checked\": False,\n",
    "                        \"offset1\": '',\n",
    "                        \"offset2\": '',\n",
    "                        \"span\": extracted_tuple['span']\n",
    "                    }\n",
    "            \n",
    "                extracted_entity_results.loc[len(extracted_entity_results)] = df_row\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "986f798aed3ca42b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# extract the concatenated results strings into a new line for each tuple \n",
    "for result_dict in results_list:\n",
    "    extract_ner_results(result_dict['pmid'], result_dict['prompt_id'], result_dict['result'])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba7743f3b0d9c2bd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extracted_entity_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4f69ce585b7d7eb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(extracted_entity_results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d976196e5c4e9fea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Find offsets \n",
    "\n",
    "# loop df, find each span, calculate the word length, find the indexes of each occurance \n",
    "for _, row in extracted_entity_results.iterrows():\n",
    "    pmid = row['pmid']\n",
    "    prompt_id = row['prompt_id']\n",
    "    # find the text from the original_data with the pmid\n",
    "    text = original_data.loc[original_data['pmid'] == pmid, 'text'].iloc[0]\n",
    "    \n",
    "    if not row['offset_checked'] and row['offset1'] == '':\n",
    "        span = row['span']\n",
    "        span_length = len(span)\n",
    "        span_start_indexes = [m.start() for m in re.finditer(re.escape(span), text)]\n",
    "        span_count = 0\n",
    "        \n",
    "        matching_spans = extracted_entity_results[(extracted_entity_results['pmid']==pmid) & (extracted_entity_results['prompt_id']==prompt_id) & (extracted_entity_results['span']==span) & (extracted_entity_results['offset1']=='') & (extracted_entity_results['offset_checked']==False)]\n",
    "        \n",
    "        for index, matched_span in matching_spans.iterrows(): \n",
    "            if span_start_indexes and span_count < len(span_start_indexes):\n",
    "                extracted_entity_results.loc[index, 'offset1'] = str(span_start_indexes[span_count])\n",
    "                extracted_entity_results.loc[index, 'offset2'] = str(span_start_indexes[span_count] + span_length)\n",
    "                \n",
    "                span_count = span_count + 1\n",
    "            else: \n",
    "                # Add -1 to extra or missing ones \n",
    "                extracted_entity_results.loc[index, 'offset1'] = '-1'\n",
    "                extracted_entity_results.loc[index, 'offset2'] = '-1'\n",
    "                \n",
    "            extracted_entity_results.loc[index, 'offset_checked'] = True\n",
    "            \n",
    "        # testing code\n",
    "        # test_matching_spans = extracted_entity_results[(extracted_entity_results['pmid']==pmid) & (extracted_entity_results['prompt_id']==prompt_id) & (extracted_entity_results['span']==span)]\n",
    "        # \n",
    "        # print(test_matching_spans)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c120fe53384d5e53",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extracted_entity_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dda2d6069e63a23",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "total_results = extracted_entity_results\n",
    "len(extracted_entity_results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b42da2e6058cb0d2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# extract the hallucinations\n",
    "hallucinated_results = extracted_entity_results[(extracted_entity_results['offset1'] == '-1') & (extracted_entity_results['offset2'] == '-1')]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea6419f4290dc017",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# remove hallucinations\n",
    "extracted_entity_results = extracted_entity_results[(extracted_entity_results['offset1'] != '-1') & (extracted_entity_results['offset2'] != '-1')]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe1fc86f056705bf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extracted_entity_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2f7ae68b7c12551",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(extracted_entity_results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86edcea9bf90fa98",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Step 4: Evaluation\n",
    "\n",
    "Evaluation log is found in `eval_log.tsv`"
   ],
   "metadata": {},
   "id": "5b2da4d1-bb9b-4b6a-932c-0ceecc811e2b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "evaluation_log_filepath = \"eval_log.tsv\"\n",
    "date = datetime.today().strftime('%Y-%m-%d %H:%M:%S')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9be144593924e6c6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if os.path.isfile(evaluation_log_filepath):\n",
    "    eval_log_df=pd.read_csv(evaluation_log_filepath, sep='\\t', header=0)\n",
    "else:\n",
    "    # TODO: Keep track of the variations between the runs eg: hyperparams (fixed), prompt that worked best etc. to add the metrics for result \n",
    "    # TODO: Keep track of the hallucinations like the # of entities found that got chopped off coz they can tbe mapped to the text, confabulate\n",
    "    eval_log_df = pd.DataFrame(columns=['prompt_id', 'data_type', 'true_positive', 'false_positive', 'false_negative', 'precision', 'recall', 'f1', 'hallucination_count', 'total_result_count', 'date', 'notes'])\n",
    "\n",
    "def update_eval_log(eval_prompt_id, hallucination_count, total_result_count):\n",
    "    eval_log_df.loc[len(eval_log_df.index)] = [eval_prompt_id, data_type, 0, 0, 0, 0, 0, 0, hallucination_count, total_result_count, date, 'gpt-3.5-turbo-16k']\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc89a767005e66f3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Step 5: Saving output\n",
    "\n",
    "Save output files in the following forms:\n",
    "1. `.tsv` file and `.zip` compressed folder containing the extracted entities in the following format: \n",
    "  `pmid   filename   label   offset1   offset2   span`.\n",
    "2. `.tsv` file containing the gold standard annotations in the following BRAT format: \n",
    "  `mark   label offset1 offset2   span`.\n",
    "3. `.tsv` file containing the extracted entities in the following BRAT format: \n",
    "  `mark   label offset1 offset2   span`.\n",
    "\n"
   ],
   "metadata": {},
   "id": "96d5210a-e7e4-49b5-a29d-9fe8c28a179f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# train_annotations.tsv\n",
    "# pmid\\tfilename\\tmark\\tlabel\\toffset1\\toffset2\\tspan\n",
    "\n",
    "# Read and find what other people have done \n",
    "\n",
    "# brat format for NER\n",
    "# TODO: Ask Karin about the BRAT format correctedness\n",
    "# <unique_id>   <label> <offset1> <offset2>   <span> \n",
    "def save_brat_output(brat, df_to_save=None, filename=\"./results/temp.tsv\"):\n",
    "    \n",
    "    if brat:    \n",
    "        df_to_save[\"label-offsets\"] = df_to_save.apply(\n",
    "        lambda df_row: f\"{df_row['label']} {df_row['offset1']} {df_row['offset2']}\",axis=1)\n",
    "        \n",
    "        if 'mark' not in df_to_save.columns:\n",
    "            df_to_save[\"mark\"] = df_to_save.apply(lambda df_row: f\"T{df_row.name+1}\",axis=1)\n",
    "\n",
    "        formatted_df_to_save = df_to_save.loc[:, ['mark', 'label-offsets', 'span']]\n",
    "        formatted_df_to_save.to_csv(filename, sep ='\\t', index=False, header=False)\n",
    "        \n",
    "        print(f'----BRAT----\\nOriginal data len: {len(df_to_save)}, Reformatted len: {len(formatted_df_to_save)}\\n')\n",
    "        print(f\"Reformatted data:\\n--------------------\\n\\n{formatted_df_to_save.head(5)}\\n--------------------\\n\\n\")\n",
    "        \n",
    "    if not brat:\n",
    "        formatted_df_to_save = df_to_save.loc[:, ['pmid', 'filename', 'label', 'offset1', 'offset2', 'span']]\n",
    "        formatted_df_to_save.to_csv(f\"{filename}.tsv\", sep ='\\t', index=False, header=True)\n",
    "        \n",
    "        print(f'Original data len: {len(df_to_save)}, Reformatted len: {len(formatted_df_to_save)}\\n')\n",
    "        print(f\"Reformatted data:\\n--------------------\\n\\n{formatted_df_to_save.head(5)}\\n--------------------\\n\\n\")\n",
    "    "
   ],
   "metadata": {},
   "id": "4a8ce412-2e29-4838-90a5-15c9a72e4011",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save gold annotations in BRAT format\n",
    "if gold_annotation_filepath != \"\" and generate_brat_format:\n",
    "    gold_annotations_df = pd.read_csv(gold_annotation_filepath, sep='\\t', header=0)\n",
    "    \n",
    "    gold_annotations_df = gold_annotations_df.drop(['mark'], axis=1)\n",
    "    \n",
    "    for _, prompt in prompt_library.iterrows():\n",
    "        prompt_id = prompt['prompt_id']\n",
    "        gold_annotations_filename = f'results/temp/gold/{prompt_id}_{data_type}.ann'\n",
    "        save_brat_output(True, gold_annotations_df, gold_annotations_filename)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12f364e30eacdd45",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for _, prompt in prompt_library.iterrows():\n",
    "    prompt_id = prompt['prompt_id']\n",
    "    results_subset = extracted_entity_results[(extracted_entity_results['prompt_id']==prompt_id)]\n",
    "    \n",
    "    # Save results in BRAT format\n",
    "    if generate_brat_format:\n",
    "        results_brat_filename = f'results/temp/eval/{prompt_id}_{data_type}.ann'\n",
    "        save_brat_output(True, results_subset, results_brat_filename)\n",
    "        \n",
    "    # Update eval log\n",
    "    total_result_count = len(total_results[(total_results['prompt_id']==prompt_id)])\n",
    "    hallucination_count = len(hallucinated_results[(hallucinated_results['prompt_id']==prompt_id)])\n",
    "    update_eval_log(prompt_id, hallucination_count, total_result_count)\n",
    "    \n",
    "    # Save whole result output\n",
    "    results_filename = f'results/{prompt_id}_{data_type}'\n",
    "    save_brat_output(False, results_subset, results_filename)\n",
    "    "
   ],
   "metadata": {},
   "id": "19a97533-fd35-41cd-801c-ac42721da509",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save eval_log file\n",
    "\n",
    "# /Users/milindi/Documents/Honours/Projects/honos/results/brateval/gold\n",
    "# /Users/milindi/Documents/Honours/Projects/honos/results/brateval/eval\n",
    "eval_log_df.to_csv('eval_log.tsv', sep ='\\t', index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e511cb49c654f939",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`mvn exec:java -Dexec.mainClass=au.com.nicta.csp.brateval.CompareEntities -Dexec.args=\"-e /Users/milindi/Documents/Honours/Projects/honos/results/brateval/eval -g /Users/milindi/Documents/Honours/Projects/honos/results/brateval/gold -s exact\" -X`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7828fab82708e95a"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "da22023b1ccf9313"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
