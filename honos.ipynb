{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0881c944-4aed-4da2-8e21-72412033c1ee",
   "metadata": {},
   "source": [
    "# NER using GPT-3.5\n",
    "\n",
    "### Project name: Honos\n",
    "Date: 24th May 2024\n",
    "\n",
    "Author: Milindi Kodikara | Supervisor: Professor Karin Verspoor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0337bbac-21c4-45c9-b0ca-922bfbc97b72",
   "metadata": {},
   "source": [
    "\n",
    "Before running this notebook:\n",
    "1. [Install Jupyter notebook](https://jupyter.org/install) \n",
    "\n",
    "\n",
    "2. [Setting up Azure OpenAI model](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/working-with-models?tabs=powershell#model-updates)\n",
    "\n",
    "\n",
    "3. [Setting up connection to GPT-3.5 using Azure OpenAI service](https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-python)\n",
    "        - In the Environment variables section, instead of doing what is outlined in the link, add the `API_KEY`, `API-VERSION`, `ENDPOINT` and `DEPLOYMENT-NAME` into a `.env` file in the root folder.\n",
    "        \n",
    "4. Add the correct filename paths for `data` in Step 1 and gold annotated data filename for the `evaluate()` function in Step 4. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f739ac-16fa-4a1b-9855-e8b89fdeb194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d926f35-0054-4d89-bfda-b3d3d589dcc5",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Load and pre-process data and prompt library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a7d5cb-7d77-449d-a3f1-bcfb1281883d",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 1.1: Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0a4e4-b5b7-441f-b99f-29e18f7ed302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_text.tsv\n",
    "# pmid\\tfilename\\ttext\n",
    "\n",
    "# TODO: Replace filepath for related data file\n",
    "data = pd.read_csv(\"./genovardis_train_dev/train_text.tsv\", sep='\\t', header=0)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c170e2b03454265",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: remove this after testing\n",
    "# data = data.head(2)\n",
    "\n",
    "# data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be60d3202a45483d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# clean up text by removing the appended pmid and title abstract tags at the start of each section\n",
    "\n",
    "pattern = '(?:[\\d]{1,10}\\|t\\|)(?P<title>[\\w\\W]+)(?:\\\\n[\\d]{1,20}\\|a\\|)(?P<abstract>[\\w\\W]+)'\n",
    "\n",
    "def clean_text(text):\n",
    "    matches = re.search(pattern, text)\n",
    "    reformatted_text = f'{matches.group(\"title\")}\\n{matches.group(\"abstract\")}'\n",
    "    return reformatted_text\n",
    "\n",
    "data['text'] = [clean_text(text) for text in data['text']]\n",
    "\n",
    "\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b95fb5318ed345a8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4538b77133430eb0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Step 1.2: Load prompt library\n",
    "\n",
    "Prompt id structure:\n",
    "`p_<index>_<task>_<language>_<output>`\n",
    "\n",
    "TODO: Figure out `<guideline>_<paradigm>`"
   ],
   "metadata": {},
   "id": "da74db47-221a-40f1-9e02-2fd68220b367"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt_library = pd.read_json('prompts.json')\n",
    "\n",
    "prompt_library"
   ],
   "metadata": {},
   "id": "fb1d18f8-110f-4981-bc3a-40c417cb490a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Step 1.3: Create data+prompt dataset"
   ],
   "metadata": {},
   "id": "7e17606d-7a57-4044-94b3-ef4b802af0f4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: Buff up the prompts with guidelines and examples (shots)\n",
    "# pmid prompt_id embedded_prompt\n",
    "def embed_data_in_prompts(row_data):\n",
    "    prompts = []\n",
    "    pmid = row_data['pmid']\n",
    "    data_text = row_data['text']\n",
    "    \n",
    "    for index, row_prompt in prompt_library.iterrows():\n",
    "        instruction = row_prompt['instruction']\n",
    "        prompt_text = row_prompt['text'].format(data_text)\n",
    "        # TODO: Figure out the new line characters \n",
    "        concatenated_prompt = '{}\\n\"{}\"'.format(instruction, prompt_text)\n",
    "        \n",
    "        prompt = {'prompt_id': row_prompt['prompt_id'], 'prompt': concatenated_prompt}\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    return {'pmid': pmid, 'prompts': prompts}\n"
   ],
   "metadata": {},
   "id": "6dc0bb8a-f112-432c-aee1-335dbf175970",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "embedded_prompt_data_list = [embed_data_in_prompts(row_data) for index, row_data in data.iterrows()]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e3fd79116cffa1c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embedded_prompt_data_list[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5905c131cf4c8e7b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eac6e4d1-bfb4-4c90-8b7b-d0ffc71d6313",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Setting up GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac339d-8467-4651-bc3f-72faa835033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.environ[\"API-KEY\"],  \n",
    "    api_version=os.environ[\"API-VERSION\"],\n",
    "    azure_endpoint=os.environ[\"ENDPOINT\"]\n",
    "    )\n",
    "    \n",
    "deployment_name=os.environ[\"DEPLOYMENT-NAME\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Testing the connection\n",
    "test_response = client.chat.completions.create(model=deployment_name, messages=[{\"role\": \"user\", \"content\": \"Hello, World!\"}])\n",
    "print(test_response.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efd39ba90dcb4d0b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: Ask Karin whether we should run again and again to see what gpt generates - yes! later!\n",
    "results_list = []\n",
    "def generate_results(prompt_items):\n",
    "    \n",
    "    pmid = prompt_items['pmid']\n",
    "    \n",
    "    for prompt_item in prompt_items['prompts']:\n",
    "    \n",
    "        prompt_id = prompt_item['prompt_id']\n",
    "        prompt = prompt_item['prompt']\n",
    "        \n",
    "        # TODO: Look into hyper params like temp \n",
    "        response = client.chat.completions.create(model=deployment_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        \n",
    "        response_result = response.choices[0].message.content\n",
    "        \n",
    "        results_list.append({'pmid': pmid, 'prompt_id': prompt_id, 'result': response_result})\n",
    "    \n",
    "        print(f'Prompt:\\n{prompt}\\n\\nResponse:\\n{response_result} \\n----------\\n')\n",
    "    \n",
    "    return results_list\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b71f8c1697ad95c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for embedded_prompt_data in embedded_prompt_data_list:\n",
    "    generate_results(embedded_prompt_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ff8b7c3321ce93d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac9d89026d543e42",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(results_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f90944f4d4f74661",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0ce5f688-6a65-4e58-a070-22caf6cdf95d",
   "metadata": {},
   "source": [
    "### Step 3: Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# create df from results list and data df\n",
    "# columns = pmid, prompt_id, filename, label, offset1, offset2, span\n",
    "extracted_entity_results = pd.DataFrame(columns=['pmid','prompt_id','filename','label', 'offset_checked', 'offset1','offset2','span'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81729923a951ac42",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(extracted_entity_results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b445e9eccce4b77",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "label_entity_pattern = '^(?P<label>gene|disease)\\s+(?P<span>[\\w\\W]+)$'\n",
    "\n",
    "def extract_tuple(tuple_string):\n",
    "    stripped_tuple_string = tuple_string.strip()\n",
    "    matches = re.search(label_entity_pattern, stripped_tuple_string)\n",
    "    \n",
    "    if not matches:\n",
    "        return\n",
    "    \n",
    "    label = matches.group(\"label\").strip()\n",
    "    span = matches.group(\"span\").strip()\n",
    "    \n",
    "    return {'label': label, 'span': span}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ad88bf60dafa5b0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# extract each entity from the combined result string from gpt-3.5\n",
    "# add each extracted tuple as a new row in extracted_entity_results df\n",
    "def extract_ner_results(pmid, prompt_id, result_string):\n",
    "    extracted_list = result_string.splitlines()\n",
    "    extracted_tuple_list = [ extract_tuple(result_string) for result_string in extracted_list]\n",
    "    \n",
    "    for extracted_tuple in extracted_tuple_list:\n",
    "        if extracted_tuple:\n",
    "            row = {\n",
    "                    \"pmid\": pmid,\n",
    "                    \"prompt_id\": prompt_id,\n",
    "                    \"filename\" : data.loc[data['pmid'] == pmid, 'filename'].iloc[0],\n",
    "                    \"label\": extracted_tuple['label'],\n",
    "                    \"offset_checked\": False,\n",
    "                    \"offset1\": '',\n",
    "                    \"offset2\": '',\n",
    "                    \"span\": extracted_tuple['span']\n",
    "                }\n",
    "        \n",
    "            extracted_entity_results.loc[len(extracted_entity_results)] = row\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "986f798aed3ca42b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# extract the concatenated results strings into a new line for each tuple \n",
    "for result_dict in results_list:\n",
    "    extract_ner_results(result_dict['pmid'], result_dict['prompt_id'], result_dict['result'])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba7743f3b0d9c2bd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extracted_entity_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4f69ce585b7d7eb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(extracted_entity_results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d976196e5c4e9fea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Find offsets \n",
    "\n",
    "# loop df, find each span, calculate the word length, find the indexes of each occurance \n",
    "for _, row in extracted_entity_results.iterrows():\n",
    "    pmid = row['pmid']\n",
    "    prompt_id = row['prompt_id']\n",
    "    text = data.loc[data['pmid'] == pmid, 'text'].iloc[0]\n",
    "    \n",
    "    if not row['offset_checked'] and row['offset1'] == '':\n",
    "        span = row['span']\n",
    "        span_length = len(span)\n",
    "        \n",
    "        span_start_indexes = [m.start() for m in re.finditer(span, text)]\n",
    "        span_count = 0\n",
    "        \n",
    "        matching_spans = extracted_entity_results[(extracted_entity_results['pmid']==pmid) & (extracted_entity_results['prompt_id']==prompt_id) & (extracted_entity_results['span']==span) & (extracted_entity_results['offset1']=='') & (extracted_entity_results['offset_checked']==False)]\n",
    "        \n",
    "        for index, matched_span in matching_spans.iterrows(): \n",
    "            if span_count < len(span_start_indexes):\n",
    "                extracted_entity_results.loc[index, 'offset1'] = str(span_start_indexes[span_count])\n",
    "                extracted_entity_results.loc[index, 'offset2'] = str(span_start_indexes[span_count] + (span_length - 1))\n",
    "                \n",
    "                span_count = span_count + 1\n",
    "            else: \n",
    "                # Add -1 to extra or missing ones \n",
    "                extracted_entity_results.loc[index, 'offset1'] = '-1'\n",
    "                extracted_entity_results.loc[index, 'offset2'] = '-1'\n",
    "                \n",
    "            extracted_entity_results.loc[index, 'offset_checked'] = True\n",
    "            \n",
    "        # testing code\n",
    "        # test_matching_spans = extracted_entity_results[(extracted_entity_results['pmid']==pmid) & (extracted_entity_results['prompt_id']==prompt_id) & (extracted_entity_results['span']==span)]\n",
    "        # \n",
    "        # print(test_matching_spans)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c120fe53384d5e53",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extracted_entity_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dda2d6069e63a23",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(extracted_entity_results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b42da2e6058cb0d2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fe1fc86f056705bf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Step 4: Evaluation\n",
    "\n",
    "*Skip this part for evaluation dataset as there is no gold standard data to compare against.*"
   ],
   "metadata": {},
   "id": "5b2da4d1-bb9b-4b6a-932c-0ceecc811e2b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# train_annotations.tsv\n",
    "# pmid\\tfilename\\tmark\\tlabel\\toffset1\\toffset2\\tspan\n",
    "\n",
    "# TODO: Keep track of the variations between the runs eg: hyperparams (fixed), prompt that worked best etc. to add the metrics for result \n",
    "# Read and find what other people have done \n",
    "\n",
    "# brat format for NER\n",
    "# <unique_id>   <label>  <offset1> <offset2>   <span> \n",
    "def bratify(eval_filepath=None, results=None):\n",
    "    if eval_filepath is not None:\n",
    "        \n",
    "        gold_standard_annotations = pd.read_csv(eval_filepath, sep='\\t', header=0)\n",
    "        # TODO: Get gold standard data in brat formation for evaluation\n",
    "        print(gold_standard_annotations.sample(5))\n",
    "        # TODO: Save file in desired output file     \n",
    "        \n",
    "    if results is not None:\n",
    "        # TODO: Get results in the brat format for evaluation\n",
    "        # TODO: Remove extra whitespaces and new lines from the response for JSON format\n",
    "        # TODO: Extract each new line as a row in the results \n",
    "        # formatted_response = re.sub('[^\\S\\t]', '', response.choices[0].message.content)\n",
    "        results = results\n",
    "        # TODO: Save file in desired output format\n",
    "    "
   ],
   "metadata": {},
   "id": "4a8ce412-2e29-4838-90a5-15c9a72e4011",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: Replace filepath of to convert to brat format\n",
    "bratify(\"./genovardis_train_dev/train_annotation.tsv\")"
   ],
   "metadata": {},
   "id": "19a97533-fd35-41cd-801c-ac42721da509",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# https://github.com/READ-BioMed/brateval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bab3531e682749e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Step 5: Saving output\n",
    "\n",
    "`.tsv` file containing the annotations in the following format: \n",
    "\n",
    "`pmid   filename   label   offset1   offset2   span`.\n",
    "\n"
   ],
   "metadata": {},
   "id": "96d5210a-e7e4-49b5-a29d-9fe8c28a179f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Extracting results of a specific prompt\n",
    "def save_output(prompt_id):\n",
    "    extracted_entity_results_subset = extracted_entity_results[(extracted_entity_results['prompt_id']==prompt_id)]\n",
    "    extracted_entity_results_subset = extracted_entity_results_subset.drop(['prompt_id', 'offset_checked'], axis=1)\n",
    "    print(f'Original len: {len(extracted_entity_results)}, subset len: {len(extracted_entity_results_subset)}\\n\\n')\n",
    "    print('Sample:\\n', extracted_entity_results_subset.sample(5))\n",
    "    \n",
    "    # get results for tsv in the format\n",
    "    # `pmid   filename   label   offset1   offset2   span`.\n",
    "    filename = f'genovardis_{prompt_id}.tsv'\n",
    "    extracted_entity_results_subset.to_csv(filename, sep ='\\t', index=False, header=True)\n",
    "    \n",
    "    print(f'\\nSaved to {filename}\\n------------\\n')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f324b34b436971e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54faf207-facd-4b75-b86a-04b6f71aa0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, prompt in prompt_library.iterrows():\n",
    "   save_output(prompt['prompt_id']) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
